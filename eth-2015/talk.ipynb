{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'start_slideshow_at': 'selected', u'theme': 'sky', u'transition': 'zoom'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.html.services.config import ConfigManager\n",
    "from IPython.utils.path import locate_profile\n",
    "cm = ConfigManager(profile_dir=locate_profile(get_ipython().profile))\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'sky',\n",
    "              'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Tensor networks:  applications and algorithms\n",
    "#####Ivan Oseledets, Skolkovo Institute of Science and Technology \n",
    "##### oseledets.github.io, i.oseledets@skoltech.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This talk\n",
    "\n",
    "You can take a look at it here:\n",
    "\n",
    "http://goo.gl/9O7t0f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skoltech\n",
    "- Founded in 2011 as a new Western-style university near Moscow http://faculty.skoltech.ru\n",
    "- In collaboration with MIT\n",
    "- No departments: priority areas \"IT, Bio, Space and Nuclear\"\n",
    "- At the moment, 160 master students, 30 professors, 40 postdocs, 50 PhD studens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Talk outline\n",
    "The main goal is to show, that tensor networks are around in different communities, sharing **similar algorithms** but different applications.\n",
    "\n",
    "- What is a tensor and motivation for working with them\n",
    "- Superposition of functions and sum-product networks (+their learning)\n",
    "- Tree tensor networks and numerical linear algebra\n",
    "- Optimization methods on manifolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a tensor\n",
    "A tensor is a $d$-way array $A(i_1, \\ldots, i_d)$.  \n",
    "\n",
    "**It may take a lot of memory to store it!**\n",
    "\n",
    "It may come from many different applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation \n",
    "<font size=6.5>\n",
    "  Say we want to\n",
    "- Denoize an image $X$;\n",
    "- Generate a new image of a horse;\n",
    "- Fill the gaps (image completion).  \n",
    "\n",
    "\n",
    "To do it, we may build a probabilistic model of our images:  \n",
    "\n",
    "  $$\n",
    "  P: 256^{1000 \\times 1000} \\mapsto [0, 1]\n",
    "  $$\n",
    "  Now we can\n",
    "- Find the image that is close to $X$ and have high probability;\n",
    "- Generate new image from the distribution $P$;\n",
    "- Find the most probable values of hidden pixels.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Three dimensions\n",
    "  Random variable $\\vec{x} = (x_1, x_2, x_3)$, $x_j \\in \\{1, 2\\}$:\n",
    "- Estimate all 8 parameters from the data:\n",
    "\n",
    "  $$\n",
    "  P(\\vec{x}) := \\frac{1}{N} \\sum_{i=1}^N [\\vec{x} = \\vec{x}^i]\n",
    "  $$\n",
    " \n",
    "- Just store it explicitly: $P \\in \\mathbb{R}^{2 \\times 2 \\times 2}$.\n",
    " \n",
    "  **Million dimensions.**  \n",
    "  \n",
    "  How to estimate and store $256^{1000 000}$ parameters (probabilities)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other examples where tensors arises\n",
    "<font size=5.5>\n",
    "- Data cubes (multivariate factor analysis)\n",
    "- Discretization of a function $f(x_1, \\ldots, x_d)$ on a tensor-product grid (multivariate function approximation)\n",
    "- High-dimensional PDEs and fine grids (the idea of **quantization**)\n",
    "- The index $(i_1, \\ldots, i_d)$ can define **the state** of the system, and the element of the tensor -- **the probability** of being in such state\n",
    "- Quantum spin systems (Schrodinger equation)\n",
    "- Biochemical networks (chemical master equation)\n",
    "- Hidden Markov Models\n",
    "- Probabilistic context-free grammars (probabilities of the symbols)\n",
    "- Weighted finite automata\n",
    "- Classification problems (i.e., $(i_1, \\ldots, i_d)$ is a **feature vector**)\n",
    "- Markov random fields/Graphical models\n",
    "- Deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different names, techniques and communities\n",
    "<font size=6.0>\n",
    "Many algorithms and methods are known in different communities:\n",
    "\n",
    "- Numerical linear algebra/numerical analysis: tensor trains, H-Tucker, Tucker, canonical decomposition\n",
    "- Solid state physics/quantum information: matrix product states/tensor networks, uniform MPS\n",
    "- Statistical physics: tranfer matrices\n",
    "- Markov random fields: belief propagation\n",
    "- Hidden Markov Models: spectral methods, EM-algorithm\n",
    "- Weighted finite automata\n",
    "- Sum-product network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor formats\n",
    "We need a tensor format (or multivariate function representation) that is:\n",
    "- **Easy to evaluate** (inference).\n",
    "- **Flexible** (i.e. the class of interesting tensors represented in this form is as large as possible)\n",
    "- **Fast learning** (i.e. the structure and parameters can be recovered from the observed data in reasonable time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Idea: superposition of functions\n",
    "A superposition of simple functions is easy to evaluate!\n",
    "<font size=6.5>\n",
    "**Kolmogorov superposition theorem**:\n",
    "\n",
    "Every continious function from $[0, 1]^d \\rightarrow \\mathbb{R}$ can be written as a superposition of smaller-dimensional functions\n",
    "\n",
    "$$f(x_1, \\ldots, x_d) = \\sum_{n=0}^{2d} g_n(\\xi(x_1 + na, \\ldots, x_d + na)), $$\n",
    "$$\\xi(x_1 + na, \\ldots, x_d + na) = \\sum_{i=1}^d \\lambda_i \\psi(x_i + na) $$\n",
    "\n",
    "Not constructive, but more **instructive**: what we can only compute is the superposition of simple functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sum and products\n",
    "<font size=6.7>\n",
    "The simplest construction is the  combination  of **weighted sum** operation and **product operation**\n",
    "\n",
    "Now we will follow the idea of Sum-product network (SPN) by [Hoifung Poon and Pedro Domingos](http://turing.cs.washington.edu/papers/uai11-poon.pdf)\n",
    "\n",
    "We can start from **simple univariate functions** (or **distributions**), and the state the following axioms.\n",
    "\n",
    "- Univariate functions (gaussian, polynomials...) are SPN.\n",
    "- Product of SPN over disjoint sets is an SPN, i.e. $f(x_1, x_2) g(x_3, x_5)$.\n",
    "- Sum over **the same variables** is an SPN, i.e. $\\sum_{x_2} f(x_1, x_2) g(x_2, x_3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%tikz -l graphdrawing,graphs,trees,automata,quotes -f svg -s 800,500 --save /Users/ivan/work/talks-online/eth-2015/spn.svg\n",
    "\\tikzset{>=stealth}\n",
    "\\usegdlibrary{layered,trees,circular,force}\n",
    "\\graph[grow=up,  layered layout]{\n",
    "    q__\"$\\times$\",\n",
    "    s1__\"$+$\", s2__\"$+$\",s3__\"$+$\", s4__\"$+$\",\n",
    "    p1__\"$\\times$\",     p2__\"$\\times$\", p3__\"$\\times$\", p4__\"$\\times$\", p5__\"$\\times$\", 2__\"$x_1$\", 3__\"$x_1$\", 4__\"$x_1$\", \n",
    "    5__\"$x_2$\", 6__\"$x_2$\", 7__\"$x_2$\", 8__\"$x_3$\", 9__\"$x_3$\",\n",
    "    2 -> {p1}, 3 -> {p2}, 4 -> {p3}, 5 -> {p1}, 6 -> {p2},  7 -> {p3}, 8 -> {p4}, 2 -> {p4}, 9 -> {p5}, 3 -> {p5}, \n",
    "    s1 <- {p1, p2}, s2 <- {p2, p3}, s3 <- {p1, p3},  s4 <- {p4, p5}, q <- {s1, s2, s3, s4}\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SPN as a tree\n",
    "SPN can be visualized as a tree, with sum/product nodes combined in different ways.\n",
    "\n",
    "<img src='spn.svg' width=80%>\n",
    "\n",
    "Example of the SPN network for a function of three variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation behind \"sum\" nodes\n",
    "\n",
    "If the we approximate the probability distribution, then there is a natural interpretation of the **sum nodes**:\n",
    "\n",
    "It is **conditional independence**: you have two variables, and $P(x, y) \\neq P_x(x) P_y(y)$.  \n",
    "\n",
    "Assume that given certain hidden variable $z$ they are indepedent: \n",
    "\n",
    "$$P(x, y | z) = P_x(x | z) P_y( y | z),$$\n",
    "\n",
    "thus $P(x, y)$ is just a sum of products over $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SPN meaning\n",
    "Our input data is are the realizations of the $(x_1, \\ldots, x_d)$ according to the probability distribution.\n",
    "\n",
    "It is a table of size  $N_{\\mathrm{samples}} \\times d$.\n",
    "\n",
    "The nodes have a simple meaning:\n",
    "\n",
    "- Sum node gives **clusters** in the dataset\n",
    "- Product node **independent sets of variables**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Idea of learning\n",
    "\n",
    "There is a nice algorithm for learning SPN, which is based on the probability interpretation. \n",
    "\n",
    "1. Given samples from the probability distribution, establish approximate independence of two **groups of variables**.\n",
    "2. If there is no independence for any two groups, group similar instances and represent the function as a sum (and learn SPN for each subset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Special cases of SPN\n",
    "- Hierarchical Tucker decomposition (SPN is more general).\n",
    "- Thin junction trees (Hidden Markov Model/Tensor train)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using SPN to approximate functions\n",
    "But up to now it has been applied only for probability distributions!  \n",
    "\n",
    "What if we are numerical analysts and \n",
    "\n",
    "**want to approximate a multidimensional function**?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Proposed solution\n",
    "Provided that $f$ can be interpreted as a probability distribution,  \n",
    "\n",
    "sample it using **Gibbs sampler**  (or Metropolis-Hastings), and then apply the SPN method to the samples.\n",
    "\n",
    "This way we will be able to recover the network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor network\n",
    "\n",
    "Sum-product-network can be seen as a particular case of more general **tensor formats**, represented by graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematical definition of a tensor network\n",
    "\n",
    "- Given a graph, we associate **original indices** with the vertices, and **auxiliary indices** with the **edges**.\n",
    "\n",
    "- In each vertex we store a **tensor** that depends on the index in the **vertex** and in all **edges**.\n",
    "\n",
    "- We multiply these tensors and sum over all edges:  \n",
    "\n",
    "\n",
    "   $$A(i_1, \\ldots, i_d) = \\sum_{s(E)}\\prod_V G(s(V)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is it easy to work with a tensor network?\n",
    "\n",
    "**The simplest task:** evaluate an element of the tensor. \n",
    "\n",
    "The more cycles the graph of your network has, the higher is the complexity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tree Tensor Networks\n",
    "\n",
    "A special case is the case of **tree tensor networks**.  \n",
    "\n",
    "In this case, the evaluation of the element can be done by bottom-to-top tree traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%tikz -l graphdrawing,graphs,trees,automata,quotes -f svg --save /Users/ivan/work/talks-online/eth-2015/tt.svg\n",
    "\\tikzset{>=stealth}\n",
    "\\usegdlibrary{layered,trees,circular,force}\n",
    "\\graph[grow right=1.5cm, branch down]{\n",
    " {[nodes={rectangle, draw}] i1__$i_1$ -- [\"$\\alpha_1$\"] i2__$i_2$ --  [\"$\\alpha_2$\"] \n",
    "                           i3__$i_3$ -- [\"$\\alpha_3$\"]  i4__$i_4$ -- [\"$\\alpha_4$\"] i5__$i_5$}, \n",
    "    / -!- / -!- A__\"Tensor train / matrix product state network\" -!- i3;\n",
    "   \n",
    "  \n",
    "};  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor-train / matrix product state \"network\"\n",
    "<font size=6.5>\n",
    "A specific case of the tree tensor network is the **tensor train** (or matrix product state) representatation, \n",
    "\n",
    "which can be written as\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = G_1(i_1) \\ldots G_d(i_d),$$\n",
    "\n",
    "i.e. the **product of matrices, depending only on 1 index**, $G_k(i_k)$ is $r_{k-1} \\times r_k$ and $r_0 = r_d = 1$.\n",
    "\n",
    "**It depends on the ordering of the indices**.\n",
    "\n",
    "<img src='tt.svg' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Canonical format\n",
    "\n",
    "A popular choice in function approximation is the **canonical** or sum-of-products format\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) \\approx \\sum_{\\alpha=1}^r U_1(i_1, \\alpha) \\ldots U_d(i_d, \\alpha),$$\n",
    "\n",
    "i.e. sum of separable functions.\n",
    "\n",
    "Disadvantage: **it is not a stable format**: the best approximation may not exist, and may be hard to compute if we know that it exists!\n",
    "\n",
    "However, for a particular tensor $A$ **it can be very efficient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%tikz -l graphdrawing,graphs,trees,automata,quotes,graphs.standard -f svg --save /Users/ivan/work/talks-online/eth-2015/2d.svg\n",
    "\\tikzset{>=stealth}\n",
    "\\usegdlibrary{layered,trees,circular,force}\n",
    "\\graph[grid placement]{subgraph Grid_n[V={$i_1$, $i_2$, $i_3$, $i_4$, $i_5$, $i_6$, $i_7$, $i_8$, $i_9$}] };"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2D network\n",
    "For many problems, the tree or train is not sufficient. For example, for 2D images you might think that a 2D network is appropriate.\n",
    "<img src='2d.svg' width=40%>\n",
    "Example of the 2D tensor network, \n",
    "in each node we have to store 5-dimensional tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two-dimensional networks are hard\n",
    "Even evaluation of an element  is an $\\mathcal{O}(r^L)$ operation for an $L \\times L$ network.\n",
    "\n",
    "However, for the particular tensor, you may be able to compute efficiently using **approximate computations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One of the ways to efficiency\n",
    "We know a lot about efficient computations with **tensor train / tree formats**.\n",
    "\n",
    "We can:\n",
    "- Perform basic linear algebra operations (ranks increase during those)\n",
    "- Multiply the tensors element-wise (ranks multiply)\n",
    "- Do **recompression** (i.e. approximate a given tensor by another one will smaller ranks and error bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor train\n",
    "\n",
    "The TT-format \n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = G_1(i_1) \\ldots G_d(i_d),$$\n",
    "\n",
    "given the **ordering of the indices** can be characterized by the following condition:\n",
    "\n",
    "$$\\mathrm{rank}(A_k) = r_k,$$\n",
    "\n",
    "where $A_k = A(i_1 i_2 \\ldots i_k; i_{k+1} \\ldots i_d)$ is the **k-th unfolding** of the tensor.\n",
    "\n",
    "I.e. it is the **intersection of low-rank matrix manifolds**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Software\n",
    "We have a TT-Toolbox, both in MATLAB http://github.com/oseledets/TT-Toolbox and in Python http://github.com/oseledets/ttpy \n",
    "\n",
    "for\n",
    "\n",
    "- Computing the TT representation (i.e. checking if there is such a representation)\n",
    "- Performing basic operations\n",
    "- Adaptive sampling algorithms (cross approximation)\n",
    "- Optimization algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# A short illustration\n",
    "import numpy as np\n",
    "import tt\n",
    "from tt.cross import rect_cross as cross\n",
    "\n",
    "d = 20\n",
    "a = tt.ones(2, d)\n",
    "b = a + a\n",
    "#print b\n",
    "#fun = lambda x: x.sum(axis=1)\n",
    "#c = cross(fun, b)\n",
    "#c.round(1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with tensor networks using linear / tree networks\n",
    "\n",
    "We have problems that can be efficiently solved using well-established NLA techniques, so it is a good idea to use that as **building blocks**.\n",
    "\n",
    "\n",
    "Reduce these operations to operations with tensor trains, i.e. represent 2D network as a layer of 1D tensor networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical example\n",
    "<font size=6.5>\n",
    "A Markov random field is another example of **tensor network**.\n",
    "- Define an **undirected graph** $\\mathcal{G}$ with nodes corresponding to the variables (pixels of the image).\n",
    "- Define some positive functions $\\Psi_c(T_c; X, W)$ (called MRF **factors**) on the cliques of the graph $\\mathcal{G}$.\n",
    "- The model is then defined as follows:\n",
    "\t$$\n",
    "\tp(T | X, W) = \\frac{1}{Z(X, W)} \\prod\\limits_{c \\in \\mathcal{C}} \\Psi_c(T_c; X, W),\n",
    "\t$$\n",
    "\twhere $Z(X, W)$ is the normalization constant.\n",
    "\n",
    "Even if the potentials are known, $Z(X, W)$ is difficult to compute (sum of a huge array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation as a product of low TT-rank tensors\n",
    "\n",
    "Then, each factor depends only on the few variables, so its ranks are bounded.  \n",
    "\n",
    "Thus, the tensor is a product of low-rank tensors\n",
    "\n",
    "$$\n",
    "   A = A_1 \\circ A_2 \\ldots \\circ A_p,\n",
    "$$\n",
    "\n",
    "where $A_k(i_1, \\ldots, i_d) = A_{k1}(i_1) \\ldots A_{kd}(i_d)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, we use the standard trick and obtain\n",
    "<font size=6.5>\n",
    "$$\\sum_{i_1, \\ldots, i_d} A(i_1, \\ldots, i_d) = \\Big(\\sum_{i_1} (A_{11}(i_1) \\otimes A_{21}(i_1) \\otimes \\ldots \\otimes A_{p1}(i_1)\\Big)\\ldots \\Big(\\sum_{i_1} (A_{11}(i_1) \\otimes A_{2d}(i_d) \\otimes \\ldots \\otimes A_{pd}(i_d)\\Big),$$\n",
    "</font>\n",
    "i.e. is now **a product of small-rank tensors**, so we **multiply them  and compress**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing marginals\n",
    "<font size=6.0>\n",
    "One example from the paper by [Putting MRFs on a Tensor Train, A. Novikov, A. Rodomanov, A. Osokin, D. Vetrov, ICML-2014](https://www.dropbox.com/s/d479j6zocine232/Paper.pdf)\n",
    "<img width=80% src='spinglass.svg'>\n",
    "\t\t$$\\widehat{\\mathbf{P}}(\\vec{x}) = \\prod_{i=1}^n \\exp \\left ( -\\frac{1}{T} h_i x_i \\right ) \\prod_{(i, \\, j) \\in \\mathcal{E}} \\exp \\left (-\\frac{1}{T}c_{ij} x_i x_j \\right )$$\n",
    "\n",
    "Spin glass models, $T = 1$, $c_{ij} \\sim U[-f, f]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "- Tensor network is a convenient formalism\n",
    "- Not all tensor networks are **easy**\n",
    "- For some problem setups we can **recover the structure**\n",
    "- We can use linear algebra tools for trees/trains\n",
    "- We can use trees/trains to solve problems for more general tensor networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization problems with low-rank constaints\n",
    "Even if there is no good low-rank approximation, low-rank tensor formats can be used as **regularizations** or **initial approximations**.\n",
    "\n",
    "Given a certain functional $F(X)$, we restrict $X$ to the set of \"low-rank\" tensors $\\mathcal{M}$, and minimize\n",
    "\n",
    "$$F(X) \\rightarrow \\min, X \\in \\mathcal{M}$$\n",
    "\n",
    "Usually in tensor community, alternating minimization is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Alternating optimization\n",
    "\n",
    "Given $A(i_1, \\ldots, i_d) = G_1(i_1) \\ldots G_d(i_d)$ optimize over one core $G_k(i_k)$.  \n",
    "\n",
    "- Good for **quadratic functionals**, and also you can parametrize     \n",
    "    \n",
    "    $$\\mathrm{vec}(A) = Q \\mathrm{vec}(G_k),$$  \n",
    "    where $Q$ is orthonormal.\n",
    "- Bad for non-quadratic (frequent in machine learning!)\n",
    "\n",
    "Therefore, Riemanian optimization techniques are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization methods on manifolds\n",
    "\n",
    "For the low-rank manifold (matrix, TT, HT) we can efficiently compute the **projection** to the tangent space.\n",
    "\n",
    "\n",
    "The simplest is to **project the gradient onto the tangent space:**\n",
    "\n",
    "$$x_{k+1} = R(x_k + P_{\\mathcal{T}}(\\nabla F)), $$\n",
    "\n",
    "where $R$ is the mapping from the tangent space to the manifold, called **retraction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2D case \n",
    "Just to give you the feeling, in two dimensions there is a simple formula.\n",
    "\n",
    "$$X  = U S V^{\\top},$$\n",
    "\n",
    "$$P_X(Z) = Z - (I - UU^{\\top}) Z (I - VV^{\\top})$$\n",
    "\n",
    "is the projection onto the tangent space.\n",
    "\n",
    "For the multidimensional case, see [Time integration of tensor trains, C. Lubich, I. Oseledets, B. Vandereycken](http://arxiv.org/abs/1407.2042)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Manifold optimization\n",
    "- Problems with low-rank constraints can be solved efficiently using Riemannian optimization\n",
    "- A good implementation does not require the full tensor to be stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: all-subsets regression\n",
    "\n",
    "Consider the binary classification problem. \n",
    "\n",
    "Log-regression is the simplest choice: given the **feature vector** $x$, \n",
    "\n",
    "we predict the probability to observe $y_i$\n",
    "\n",
    "$$p = Z \\exp(-y_i \\langle w, x_i \\rangle).$$\n",
    "\n",
    "I.e. the predictor variable is just the linear combination of the components of the feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## All-subsets regression\n",
    "\n",
    "We can use other predictor variable, for example, select product of features (subsets) like\n",
    "\n",
    "$$w_1 x_1 + w_{125} x_1 x_2 x_5 + \\ldots $$\n",
    "\n",
    "We can code all **possible subsets** in this form by a vector of length $2^d$, or tensor of size $2 \\times \\ldots \\times 2$.\n",
    "\n",
    "(i.e. if there is $x_1$ in the term, or not). \n",
    "\n",
    "The predictor variable is then **$t = \\langle W, X \\rangle$**, where $\\langle \\cdot \\rangle$ is the scalar product of tensors.\n",
    "\n",
    "We impose low-rank constraints on the $W$, to avoid **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization problem\n",
    "<font size=6.5>\n",
    "The total loss is the sum of individual losses\n",
    "\n",
    "$$F(W) = \\sum_{k=1}^K f(y_i, \\langle X_i, W \\rangle),$$\n",
    "\n",
    "where $X_i$ is the low-rank tensor obtained from the **feature vector** $x_i$.\n",
    "\n",
    "The gradient is easily computatble:\n",
    "\n",
    "$$\\nabla F = \\sum_{k=1}^K \\frac{df}{dz}(y_i, \\langle X_i, W \\rangle) X_i,$$\n",
    "\n",
    "and **projection** onto the tangent space can be computed in a fast way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminary results\n",
    "On the problem [Otto](https://www.kaggle.com/c/otto-group-product-classification-challenge) from Kaggle, the larger the rank, the better is learning (still learning today)\n",
    "<img src='all-subsets.svg'>\n",
    "\n",
    "You have to train fast   \n",
    "(GPU implementation is necessary, as in the Deep Learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Low-rank factorization as initialization\n",
    "\n",
    "You can use low-rank factorization to initialize other optimization methods.\n",
    "\n",
    "We have successfully speeded up the convolutional neural networks by factorizing a 4D tensor into the canonical format and then **fine-tuning it**.\n",
    "\n",
    "[Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition\n",
    "Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, Victor Lempitsky, accepted at ICLR 2015](http://arxiv.org/abs/1412.6553)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Next level\" multivariate functions representations\n",
    "\n",
    "- Are based on the composition of \"simple\" functions (product, sums)\n",
    "- **Proposal**: consider superposition of low-rank functions.\n",
    "- Require fast solvers for low-rank constained optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: quantization\n",
    "\n",
    "Introduced by Oseledets for matrices and Khoromskij for vectors,  the idea of **quantization** is very powerful:\n",
    "\n",
    "Given a univariate function $f(x)$ on a uniform grid with $2^d$ points, reshape it into $2 \\times \\ldots \\times 2$  \n",
    "\n",
    "$d$-dimensional tensor and apply **TT-decomposition to it**. The is the **QTT** decomposition.\n",
    "\n",
    "It gives **logarithmic complexity** in the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo\n",
    "Consider the function $f(x, y) = \\frac{\\exp(i k r)}{r}, r = \\sqrt{x^2 + y^2}$ on a uniform grid in $[\\delta, 1]^2$ with the quantization  \n",
    "in **Cartesian coordinates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tt\n",
    "import numpy as np\n",
    "k = 50\n",
    "delta = 1e-2\n",
    "d = 20 \n",
    "n = 2 ** d\n",
    "t = np.linspace(delta, 1, n)\n",
    "t = np.reshape(t, [2] * d, \"F\")\n",
    "t_ten = tt.tensor(t, eps=1e-10)\n",
    "e = tt.ones(2, d)\n",
    "x = tt.kron(t_ten, e)\n",
    "y = tt.kron(e, t_ten)\n",
    "my_fun = lambda x: np.exp(-1j * k * np.sqrt(x[:, 0] ** 2 + x[:, 1] ** 2))/(np.sqrt(x[:, 0] ** 2 + x[:, 1] ** 2))\n",
    "z = tt.multifuncrs([x, y], my_fun, eps=1e-6, verb=False)\n",
    "plt.plot(z.r)\n",
    "plt.text(0.5, 0.5, 'k=%d, Maximal rank=%d' % (k, max(z.r)),  fontsize=14, transform=plt.gca().transAxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ranks in polar  coordinates\n",
    "\n",
    "$$x = r \\cos \\varphi, \\quad y = r \\cos \\varphi, \\quad f(r) = \\frac{\\exp(ikr)}{r}, $$\n",
    "\n",
    "all these functions have bounded QTT-rank!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: active subspaces\n",
    "Another example is to restrict the class of transformations, i.e. linear transformations.  \n",
    "\n",
    "This leads to the approach of **active subspaces**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Active subspaces (cont.)\n",
    "Given a function $f(x_1, \\ldots, x_d)$ we try reparametrize the variable $x$ into a new variable $t$ by a **linear transformation** \n",
    "\n",
    "$$x = W t.$$\n",
    "\n",
    "A natural way to go is to align the axis with the directions where the function **varies the most**:\n",
    "\n",
    "1. Compute the covariance matrix of the gradients $C = E(\\nabla f \\nabla f^{\\top})$\n",
    "2. Compute the  eigenvectors $W$ of  $C_x$\n",
    "\n",
    "This can significantly improve the separability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some examples.\n",
    "1. If the function $f$ is a function of a fewer linear combinations of original variables, then $C_x$ will be not of full rank.\n",
    "2. Gaussian function $\\exp({-x^{\\top} A x})$ is rotated into the product form!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical example from chemistry\n",
    "We have considered the potential energy surface of the water molecule in the Cartesian coordinates.  \n",
    "\n",
    "It has 9 dimensions, and active subspace rotation significantly reduces the ranks.\n",
    "\n",
    "<img width=80% src='cheb_table.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is it possible to learn the network structure?\n",
    "\n",
    "<font size=6.5>\n",
    "The main problems are:\n",
    "- How to select the network structure in the appropriate way?\n",
    "- How to learn the parameters (i.e., tensors living in each vertex) of the network, once you know its structure.\n",
    "- Learn the parameters very well with linear structures (and tensor trains) and also with tree structure (and H-Tucker). \n",
    "\n",
    "** We know particial answers in many cases**, but not in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Missing from this talk\n",
    "- Weighted finite automata $\\approx$ tensor trains with the same cores\n",
    "- Probabilistic context-free grammars "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where to start (literature)\n",
    "- [A literature survey of low‚Äêrank tensor approximation techniques L. Grasedyck, D. Kressner, C. Tobler](http://onlinelibrary.wiley.com/doi/10.1002/gamm.201310004/pdf)\n",
    "- [Tensor decompositions and applications T.G. Kolda, B. W Bader](http://epubs.siam.org/doi/pdf/10.1137/07070111X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Publications and software\n",
    "- http://oseledets.github.io -- Scientific Computing Group at Skoltech\n",
    "- http://github.com/oseledets/TT-Toolbox -- Tensor Trains in MATLAB\n",
    "- http://github.com/oseledets/ttpy -- Tensor Trains in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MMMA-2015\n",
    "In August 23-28 2015 we hold the **4-th Moscow conference in Matrix Methods in Mathematics and Applications**.  \n",
    "\n",
    "Confirmed speakers: C. Schwab, B. Benner, J. White, D. Zorin, M. Benzi, P.-A.Absil, A. Cichocki, P. Van Dooren.\n",
    "\n",
    "**Good time to visit Moscow** (i.e., due to the exchange rate drop from 35 roubles/dollar to 50 roubles/dollar). \n",
    "\n",
    "\n",
    "\n",
    "http://matrix.inm.ras.ru/mmma-2015/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
