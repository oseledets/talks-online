{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'start_slideshow_at': 'selected', u'theme': 'sky', u'transition': 'zoom'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.html.services.config import ConfigManager\n",
    "from IPython.utils.path import locate_profile\n",
    "cm = ConfigManager(profile_dir=locate_profile(get_ipython().profile))\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'sky',\n",
    "              'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Tensor networks:  applications and algorithms\n",
    "#####Ivan Oseledets, Skolkovo Institute of Science and Technology \n",
    "##### oseledets.github.io, i.oseledets@skoltech.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This talk\n",
    "\n",
    "You can take a look at it here:\n",
    "\n",
    "http://goo.gl/9O7t0f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skoltech\n",
    "- Founded in 2011 as a new Western-style university near Moscow http://faculty.skoltech.ru\n",
    "- In collaboration with MIT\n",
    "- No departments: priority areas \"IT, Bio, Space and Nuclear\"\n",
    "- At the moment, 160 master students, 30 professors, 40 postdocs, 50 PhD studens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Talk outline\n",
    "The main goal is to show, that tensor networks are around in different communities, sharing **similar algorithms** but different applications.\n",
    "\n",
    "- What is a tensor and motivation for working with them\n",
    "- Superposition of functions and sum-product networks (+their learning)\n",
    "- Tree tensor networks and numerical linear algebra\n",
    "- Optimization methods on manifolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a tensor\n",
    "A tensor is a $d$-way array $A(i_1, \\ldots, i_d)$.  \n",
    "\n",
    "**It may take a lot of memory to store it!**\n",
    "\n",
    "It may come from many different applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation \n",
    "<font size=6.5>\n",
    "  Say we want to\n",
    "- Denoize an image $X$;\n",
    "- Generate a new image of a horse;\n",
    "- Fill the gaps (image completion).  \n",
    "\n",
    "\n",
    "To do it, we may build a probabilistic model of our images:  \n",
    "\n",
    "  $$\n",
    "  P: 256^{1000 \\times 1000} \\mapsto [0, 1]\n",
    "  $$\n",
    "  Now we can\n",
    "- Find the image that is close to $X$ and have high probability;\n",
    "- Generate new image from the distribution $P$;\n",
    "- Find the most probable values of hidden pixels.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Three dimensions\n",
    "  Random variable $\\vec{x} = (x_1, x_2, x_3)$, $x_j \\in \\{1, 2\\}$:\n",
    "- Estimate all 8 parameters from the data:\n",
    "\n",
    "  $$\n",
    "  P(\\vec{x}) := \\frac{1}{N} \\sum_{i=1}^N [\\vec{x} = \\vec{x}^i]\n",
    "  $$\n",
    " \n",
    "- Just store it explicitly: $P \\in \\mathbb{R}^{2 \\times 2 \\times 2}$.\n",
    " \n",
    "  **Million dimensions.**  \n",
    "  \n",
    "  How to estimate and store $256^{1000 000}$ parameters (probabilities)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other examples where tensors arises\n",
    "<font size=5.5>\n",
    "- Data cubes (multivariate factor analysis)\n",
    "- Discretization of a function $f(x_1, \\ldots, x_d)$ on a tensor-product grid (multivariate function approximation)\n",
    "- High-dimensional PDEs and fine grids (the idea of **quantization**)\n",
    "- The index $(i_1, \\ldots, i_d)$ can define **the state** of the system, and the element of the tensor -- **the probability** of being in such state\n",
    "- Quantum spin systems (Schrodinger equation)\n",
    "- Biochemical networks (chemical master equation)\n",
    "- Hidden Markov Models\n",
    "- Probabilistic context-free grammars (probabilities of the symbols)\n",
    "- Weighted finite automata\n",
    "- Classification problems (i.e., $(i_1, \\ldots, i_d)$ is a **feature vector**)\n",
    "- Markov random fields/Graphical models\n",
    "- Deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different names, techniques and communities\n",
    "<font size=6.0>\n",
    "Many algorithms and methods are known in different communities:\n",
    "\n",
    "- Numerical linear algebra/numerical analysis: tensor trains, H-Tucker, Tucker, canonical decomposition\n",
    "- Solid state physics/quantum information: matrix product states/tensor networks, uniform MPS\n",
    "- Statistical physics: tranfer matrices\n",
    "- Markov random fields: belief propagation\n",
    "- Hidden Markov Models: spectral methods, EM-algorithm\n",
    "- Weighted finite automata\n",
    "- Sum-product network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor formats\n",
    "We need a tensor format (or multivariate function representation) that is:\n",
    "- **Easy to evaluate** (inference).\n",
    "- **Flexible** (i.e. the class of interesting tensors represented in this form is as large as possible)\n",
    "- **Fast learning** (i.e. the structure and parameters can be recovered from the observed data in reasonable time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Idea: superposition of functions\n",
    "A superposition of simple functions is easy to evaluate!\n",
    "<font size=6.5>\n",
    "**Kolmogorov superposition theorem**:\n",
    "\n",
    "Every continious function from $[0, 1]^d \\rightarrow \\mathbb{R}$ can be written as a superposition of smaller-dimensional functions\n",
    "\n",
    "$$f(x_1, \\ldots, x_d) = \\sum_{n=0}^{2d} g_n(\\xi(x_1 + na, \\ldots, x_d + na)), $$\n",
    "$$\\xi(x_1 + na, \\ldots, x_d + na) = \\sum_{i=1}^d \\lambda_i \\psi(x_i + na) $$\n",
    "\n",
    "Not constructive, but more **instructive**: what we can only compute is the superposition of simple functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sum and products\n",
    "<font size=6.7>\n",
    "The simplest construction is the  combination  of **weighted sum** operation and **product operation**\n",
    "\n",
    "Now we will follow the idea of Sum-product network (SPN) by [Hoifung Poon and Pedro Domingos](http://turing.cs.washington.edu/papers/uai11-poon.pdf)\n",
    "\n",
    "We can start from **simple univariate functions** (or **distributions**), and the state the following axioms.\n",
    "\n",
    "- Univariate functions (gaussian, polynomials...) are SPN.\n",
    "- Product of SPN over disjoint sets is an SPN, i.e. $f(x_1, x_2) g(x_3, x_5)$.\n",
    "- Sum over **the same variables** is an SPN, i.e. $\\sum_{x_2} f(x_1, x_2) g(x_2, x_3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%tikz -l graphdrawing,graphs,trees,automata,quotes -f svg -s 800,500 --save /Users/ivan/work/talks-online/eth-2015/spn.svg\n",
    "\\tikzset{>=stealth}\n",
    "\\usegdlibrary{layered,trees,circular,force}\n",
    "\\graph[grow=up,  layered layout]{\n",
    "    q__\"$\\times$\",\n",
    "    s1__\"$+$\", s2__\"$+$\",s3__\"$+$\", s4__\"$+$\",\n",
    "    p1__\"$\\times$\",     p2__\"$\\times$\", p3__\"$\\times$\", p4__\"$\\times$\", p5__\"$\\times$\", 2__\"$x_1$\", 3__\"$x_1$\", 4__\"$x_1$\", \n",
    "    5__\"$x_2$\", 6__\"$x_2$\", 7__\"$x_2$\", 8__\"$x_3$\", 9__\"$x_3$\",\n",
    "    2 -> {p1}, 3 -> {p2}, 4 -> {p3}, 5 -> {p1}, 6 -> {p2},  7 -> {p3}, 8 -> {p4}, 2 -> {p4}, 9 -> {p5}, 3 -> {p5}, \n",
    "    s1 <- {p1, p2}, s2 <- {p2, p3}, s3 <- {p1, p3},  s4 <- {p4, p5}, q <- {s1, s2, s3, s4}\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SPN as a tree\n",
    "SPN can be visualized as a tree, with sum/product nodes combined in different ways.\n",
    "\n",
    "<img src='spn.svg' width=80%>\n",
    "\n",
    "Example of the SPN network for a function of three variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation behind \"sum\" nodes\n",
    "\n",
    "If the we approximate the probability distribution, then there is a natural interpretation of the **sum nodes**:\n",
    "\n",
    "It is **conditional independence**: you have two variables, and $P(x, y) \\neq P_x(x) P_y(y)$.  \n",
    "\n",
    "Assume that given certain hidden variable $z$ they are indepedent: \n",
    "\n",
    "$$P(x, y | z) = P_x(x | z) P_y( y | z),$$\n",
    "\n",
    "thus $P(x, y)$ is just a sum of products over $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SPN meaning\n",
    "Our input data is are the realizations of the $(x_1, \\ldots, x_d)$ according to the probability distribution.\n",
    "\n",
    "It is a table of size  $N_{\\mathrm{samples}} \\times d$.\n",
    "\n",
    "The nodes have a simple meaning:\n",
    "\n",
    "- Sum node gives **clusters** in the dataset\n",
    "- Product node **independent sets of variables**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Idea of learning\n",
    "\n",
    "There is a nice algorithm for learning SPN, which is based on the probability interpretation. \n",
    "\n",
    "1. Given samples from the probability distribution, establish approximate independence of two **groups of variables**.\n",
    "2. If there is no independence for any two groups, group similar instances and represent the function as a sum (and learn SPN for each subset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Special cases of SPN\n",
    "- Hierarchical Tucker decomposition (SPN is more general).\n",
    "- Thin junction trees (Hidden Markov Model/Tensor train)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using SPN to approximate functions\n",
    "But up to now it has been applied only for probability distributions!  \n",
    "\n",
    "What if we are numerical analysts and \n",
    "\n",
    "**want to approximate a multidimensional function**?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Proposed solution\n",
    "Provided that $f$ can be interpreted as a probability distribution,  \n",
    "\n",
    "sample it using **Gibbs sampler**  (or Metropolis-Hastings), and then apply the SPN method to the samples.\n",
    "\n",
    "This way we will be able to recover the network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor network\n",
    "\n",
    "Sum-product-network can be seen as a particular case of more general **tensor formats**, represented by graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematical definition of a tensor network\n",
    "\n",
    "- Given a graph, we associate **original indices** with the vertices, and **auxiliary indices** with the **edges**.\n",
    "\n",
    "- In each vertex we store a **tensor** that depends on the index in the **vertex** and in all **edges**.\n",
    "\n",
    "- We multiply these tensors and sum over all edges:  \n",
    "\n",
    "\n",
    "   $$A(i_1, \\ldots, i_d) = \\sum_{s(E)}\\prod_V G(s(V)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is it easy to work with a tensor network?\n",
    "\n",
    "**The simplest task:** evaluate an element of the tensor. \n",
    "\n",
    "The more cycles the graph of your network has, the higher is the complexity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tree Tensor Networks\n",
    "\n",
    "A special case is the case of **tree tensor networks**.  \n",
    "\n",
    "In this case, the evaluation of the element can be done by bottom-to-top tree traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%tikz -l graphdrawing,graphs,trees,automata,quotes -f svg --save /Users/ivan/work/talks-online/eth-2015/tt.svg\n",
    "\\tikzset{>=stealth}\n",
    "\\usegdlibrary{layered,trees,circular,force}\n",
    "\\graph[grow right=1.5cm, branch down]{\n",
    " {[nodes={rectangle, draw}] i1__$i_1$ -- [\"$\\alpha_1$\"] i2__$i_2$ --  [\"$\\alpha_2$\"] \n",
    "                           i3__$i_3$ -- [\"$\\alpha_3$\"]  i4__$i_4$ -- [\"$\\alpha_4$\"] i5__$i_5$}, \n",
    "    / -!- / -!- A__\"Tensor train / matrix product state network\" -!- i3;\n",
    "   \n",
    "  \n",
    "};  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor-train / matrix product state \"network\"\n",
    "<font size=6.5>\n",
    "A specific case of the tree tensor network is the **tensor train** (or matrix product state) representatation, \n",
    "\n",
    "which can be written as\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = G_1(i_1) \\ldots G_d(i_d),$$\n",
    "\n",
    "i.e. the **product of matrices, depending only on 1 index**, $G_k(i_k)$ is $r_{k-1} \\times r_k$ and $r_0 = r_d = 1$.\n",
    "\n",
    "**It depends on the ordering of the indices**.\n",
    "\n",
    "<img src='tt.svg' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Canonical format\n",
    "\n",
    "A popular choice in function approximation is the **canonical** or sum-of-products format\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) \\approx \\sum_{\\alpha=1}^r U_1(i_1, \\alpha) \\ldots U_d(i_d, \\alpha),$$\n",
    "\n",
    "i.e. sum of separable functions.\n",
    "\n",
    "Disadvantage: **it is not a stable format**: the best approximation may not exist, and may be hard to compute if we know that it exists!\n",
    "\n",
    "However, for a particular tensor $A$ **it can be very efficient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%tikz -l graphdrawing,graphs,trees,automata,quotes,graphs.standard -f svg --save /Users/ivan/work/talks-online/eth-2015/2d.svg\n",
    "\\tikzset{>=stealth}\n",
    "\\usegdlibrary{layered,trees,circular,force}\n",
    "\\graph[grid placement]{subgraph Grid_n[V={$i_1$, $i_2$, $i_3$, $i_4$, $i_5$, $i_6$, $i_7$, $i_8$, $i_9$}] };"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2D network\n",
    "For many problems, the tree or train is not sufficient. For example, for 2D images you might think that a 2D network is appropriate.\n",
    "<img src='2d.svg' width=40%>\n",
    "Example of the 2D tensor network, \n",
    "in each node we have to store 5-dimensional tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two-dimensional networks are hard\n",
    "Even evaluation of an element  is an $\\mathcal{O}(r^L)$ operation for an $L \\times L$ network.\n",
    "\n",
    "However, for the particular tensor, you may be able to compute efficiently using **approximate computations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One of the ways to efficiency\n",
    "We know a lot about efficient computations with **tensor train / tree formats**.\n",
    "\n",
    "We can:\n",
    "- Perform basic linear algebra operations (ranks increase during those)\n",
    "- Multiply the tensors element-wise (ranks multiply)\n",
    "- Do **recompression** (i.e. approximate a given tensor by another one will smaller ranks and error bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor train\n",
    "\n",
    "The TT-format \n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = G_1(i_1) \\ldots G_d(i_d),$$\n",
    "\n",
    "given the **ordering of the indices** can be characterized by the following condition:\n",
    "\n",
    "$$\\mathrm{rank}(A_k) = r_k,$$\n",
    "\n",
    "where $A_k = A(i_1 i_2 \\ldots i_k; i_{k+1} \\ldots i_d)$ is the **k-th unfolding** of the tensor.\n",
    "\n",
    "I.e. it is the **intersection of low-rank matrix manifolds**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Software\n",
    "We have a TT-Toolbox, both in MATLAB http://github.com/oseledets/TT-Toolbox and in Python http://github.com/oseledets/ttpy \n",
    "\n",
    "for\n",
    "\n",
    "- Computing the TT representation (i.e. checking if there is such a representation)\n",
    "- Performing basic operations\n",
    "- Adaptive sampling algorithms (cross approximation)\n",
    "- Optimization algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swp: 0/9 er_rel = 8.1e-01 er_abs = 8.5e+03 erank = 5.7 fun_eval: 1100\n",
      "swp: 1/9 er_rel = 9.5e-16 er_abs = 1.0e-11 erank = 9.1 fun_eval: 5036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "This is a 20-dimensional tensor \n",
       "r(0)=1, n(0)=2 \n",
       "r(1)=2, n(1)=2 \n",
       "r(2)=2, n(2)=2 \n",
       "r(3)=2, n(3)=2 \n",
       "r(4)=2, n(4)=2 \n",
       "r(5)=2, n(5)=2 \n",
       "r(6)=2, n(6)=2 \n",
       "r(7)=2, n(7)=2 \n",
       "r(8)=2, n(8)=2 \n",
       "r(9)=2, n(9)=2 \n",
       "r(10)=2, n(10)=2 \n",
       "r(11)=2, n(11)=2 \n",
       "r(12)=2, n(12)=2 \n",
       "r(13)=2, n(13)=2 \n",
       "r(14)=2, n(14)=2 \n",
       "r(15)=2, n(15)=2 \n",
       "r(16)=2, n(16)=2 \n",
       "r(17)=2, n(17)=2 \n",
       "r(18)=2, n(18)=2 \n",
       "r(19)=2, n(19)=2 \n",
       "r(20)=1 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A short illustration\n",
    "import numpy as np\n",
    "import tt\n",
    "from tt.cross import rect_cross as cross\n",
    "\n",
    "d = 20\n",
    "a = tt.ones(2, d)\n",
    "b = a + a\n",
    "#print b.round(1e-6)\n",
    "fun = lambda x: x.sum(axis=1)\n",
    "c = cross(fun, b, eps=1e-6)\n",
    "c.round(1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with tensor networks using linear / tree networks\n",
    "\n",
    "We have problems that can be efficiently solved using well-established NLA techniques, so it is a good idea to use that as **building blocks**.\n",
    "\n",
    "\n",
    "Reduce these operations to operations with tensor trains, i.e. represent 2D network as a layer of 1D tensor networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical example\n",
    "<font size=6.5>\n",
    "A Markov random field is another example of **tensor network**.\n",
    "- Define an **undirected graph** $\\mathcal{G}$ with nodes corresponding to the variables (pixels of the image).\n",
    "- Define some positive functions $\\Psi_c(T_c; X, W)$ (called MRF **factors**) on the cliques of the graph $\\mathcal{G}$.\n",
    "- The model is then defined as follows:\n",
    "\t$$\n",
    "\tp(T | X, W) = \\frac{1}{Z(X, W)} \\prod\\limits_{c \\in \\mathcal{C}} \\Psi_c(T_c; X, W),\n",
    "\t$$\n",
    "\twhere $Z(X, W)$ is the normalization constant.\n",
    "\n",
    "Even if the potentials are known, $Z(X, W)$ is difficult to compute (sum of a huge array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation as a product of low TT-rank tensors\n",
    "\n",
    "Then, each factor depends only on the few variables, so its ranks are bounded.  \n",
    "\n",
    "Thus, the tensor is a product of low-rank tensors\n",
    "\n",
    "$$\n",
    "   A = A_1 \\circ A_2 \\ldots \\circ A_p,\n",
    "$$\n",
    "\n",
    "where $A_k(i_1, \\ldots, i_d) = A_{k1}(i_1) \\ldots A_{kd}(i_d)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, we use the standard trick and obtain\n",
    "<font size=6.5>\n",
    "$$\\sum_{i_1, \\ldots, i_d} A(i_1, \\ldots, i_d) = \\Big(\\sum_{i_1} (A_{11}(i_1) \\otimes A_{21}(i_1) \\otimes \\ldots \\otimes A_{p1}(i_1)\\Big)\\ldots \\Big(\\sum_{i_1} (A_{11}(i_1) \\otimes A_{2d}(i_d) \\otimes \\ldots \\otimes A_{pd}(i_d)\\Big),$$\n",
    "</font>\n",
    "i.e. is now **a product of small-rank tensors**, so we **multiply them  and compress**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing marginals\n",
    "<font size=6.0>\n",
    "One example from the paper by [Putting MRFs on a Tensor Train, A. Novikov, A. Rodomanov, A. Osokin, D. Vetrov, ICML-2014](https://www.dropbox.com/s/d479j6zocine232/Paper.pdf)\n",
    "<img width=80% src='spinglass.svg'>\n",
    "\t\t$$\\widehat{\\mathbf{P}}(\\vec{x}) = \\prod_{i=1}^n \\exp \\left ( -\\frac{1}{T} h_i x_i \\right ) \\prod_{(i, \\, j) \\in \\mathcal{E}} \\exp \\left (-\\frac{1}{T}c_{ij} x_i x_j \\right )$$\n",
    "\n",
    "Spin glass models, $T = 1$, $c_{ij} \\sim U[-f, f]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "- Tensor network is a convenient formalism\n",
    "- Not all tensor networks are **easy**\n",
    "- For some problem setups we can **recover the structure**\n",
    "- We can use linear algebra tools for trees/trains\n",
    "- We can use trees/trains to solve problems for more general tensor networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization problems with low-rank constaints\n",
    "Even if there is no good low-rank approximation, low-rank tensor formats can be used as **regularizations** or **initial approximations**.\n",
    "\n",
    "Given a certain functional $F(X)$, we restrict $X$ to the set of \"low-rank\" tensors $\\mathcal{M}$, and minimize\n",
    "\n",
    "$$F(X) \\rightarrow \\min, X \\in \\mathcal{M}$$\n",
    "\n",
    "Usually in tensor community, alternating minimization is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Alternating optimization\n",
    "\n",
    "Given $A(i_1, \\ldots, i_d) = G_1(i_1) \\ldots G_d(i_d)$ optimize over one core $G_k(i_k)$.  \n",
    "\n",
    "- Good for **quadratic functionals**, and also you can parametrize     \n",
    "    \n",
    "    $$\\mathrm{vec}(A) = Q \\mathrm{vec}(G_k),$$  \n",
    "    where $Q$ is orthonormal.\n",
    "- Bad for non-quadratic (frequent in machine learning!)\n",
    "\n",
    "Therefore, Riemanian optimization techniques are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization methods on manifolds\n",
    "\n",
    "For the low-rank manifold (matrix, TT, HT) we can efficiently compute the **projection** to the tangent space.\n",
    "\n",
    "\n",
    "The simplest is to **project the gradient onto the tangent space:**\n",
    "\n",
    "$$x_{k+1} = R(x_k + P_{\\mathcal{T}}(\\nabla F)), $$\n",
    "\n",
    "where $R$ is the mapping from the tangent space to the manifold, called **retraction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2D case \n",
    "Just to give you the feeling, in two dimensions there is a simple formula.\n",
    "\n",
    "$$X  = U S V^{\\top},$$\n",
    "\n",
    "$$P_X(Z) = Z - (I - UU^{\\top}) Z (I - VV^{\\top})$$\n",
    "\n",
    "is the projection onto the tangent space.\n",
    "\n",
    "For the multidimensional case, see [Time integration of tensor trains, C. Lubich, I. Oseledets, B. Vandereycken](http://arxiv.org/abs/1407.2042)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Manifold optimization\n",
    "- Problems with low-rank constraints can be solved efficiently using Riemannian optimization\n",
    "- A good implementation does not require the full tensor to be stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: all-subsets regression\n",
    "\n",
    "Consider the binary classification problem. \n",
    "\n",
    "Log-regression is the simplest choice: given the **feature vector** $x$, \n",
    "\n",
    "we predict the probability to observe $y_i$\n",
    "\n",
    "$$p = Z \\exp(-y_i \\langle w, x_i \\rangle).$$\n",
    "\n",
    "I.e. the predictor variable is just the linear combination of the components of the feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## All-subsets regression\n",
    "\n",
    "We can use other predictor variable, for example, select product of features (subsets) like\n",
    "\n",
    "$$w_1 x_1 + w_{125} x_1 x_2 x_5 + \\ldots $$\n",
    "\n",
    "We can code all **possible subsets** in this form by a vector of length $2^d$, or tensor of size $2 \\times \\ldots \\times 2$.\n",
    "\n",
    "(i.e. if there is $x_1$ in the term, or not). \n",
    "\n",
    "The predictor variable is then **$t = \\langle W, X \\rangle$**, where $\\langle \\cdot \\rangle$ is the scalar product of tensors.\n",
    "\n",
    "We impose low-rank constraints on the $W$, to avoid **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization problem\n",
    "<font size=6.5>\n",
    "The total loss is the sum of individual losses\n",
    "\n",
    "$$F(W) = \\sum_{k=1}^K f(y_i, \\langle X_i, W \\rangle),$$\n",
    "\n",
    "where $X_i$ is the low-rank tensor obtained from the **feature vector** $x_i$.\n",
    "\n",
    "The gradient is easily computatble:\n",
    "\n",
    "$$\\nabla F = \\sum_{k=1}^K \\frac{df}{dz}(y_i, \\langle X_i, W \\rangle) X_i,$$\n",
    "\n",
    "and **projection** onto the tangent space can be computed in a fast way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminary results\n",
    "On the problem [Otto](https://www.kaggle.com/c/otto-group-product-classification-challenge) from Kaggle, the larger the rank, the better is learning (still learning today)\n",
    "<img src='all-subsets.svg'>\n",
    "\n",
    "You have to train fast   \n",
    "(GPU implementation is necessary, as in the Deep Learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Low-rank factorization as initialization\n",
    "\n",
    "You can use low-rank factorization to initialize other optimization methods.\n",
    "\n",
    "We have successfully speeded up the convolutional neural networks by factorizing a 4D tensor into the canonical format and then **fine-tuning it**.\n",
    "\n",
    "[Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition\n",
    "Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, Victor Lempitsky, accepted at ICLR 2015](http://arxiv.org/abs/1412.6553)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Next level\" multivariate functions representations\n",
    "\n",
    "- Are based on the composition of \"simple\" functions (product, sums)\n",
    "- **Proposal**: consider superposition of low-rank functions.\n",
    "- Require fast solvers for low-rank constained optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: quantization\n",
    "\n",
    "Introduced by Oseledets for matrices and Khoromskij for vectors,  the idea of **quantization** is very powerful:\n",
    "\n",
    "Given a univariate function $f(x)$ on a uniform grid with $2^d$ points, reshape it into $2 \\times \\ldots \\times 2$  \n",
    "\n",
    "$d$-dimensional tensor and apply **TT-decomposition to it**. The is the **QTT** decomposition.\n",
    "\n",
    "It gives **logarithmic complexity** in the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo\n",
    "Consider the function $f(x, y) = \\frac{\\exp(i k r)}{r}, r = \\sqrt{x^2 + y^2}$ on a uniform grid in $[\\delta, 1]^2$ with the quantization  \n",
    "in **Cartesian coordinates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x111da2cd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYZGV5/vHvPeyLOhBgGAEz6C9sBh1AFgOEFgQHo7ij\nEAgiIjGKYIKIxmUiKkoEUXCJYdCBKC4oBDQIw9IKERiWGUBGNoHIsAwgCIwM+/P7430Lip5eqrur\n6j116v5cV1/TVXXq1N1nup869Zxz3lcRgZmZ1cuU0gHMzKz9XNzNzGrIxd3MrIZc3M3MasjF3cys\nhlzczcxqaNTiLmlVSVdIWihpkaRj8v2zJS2WtCB/zepOXDMza4XGOs9d0uoR8ZikFYFLgSOA3YBH\nI+L4LmQ0M7NxGrMtExGP5W9XBlYAHsq31alQZmY2OWMWd0lTJC0ElgAXR8QN+aFDJV0raY6kqR1N\naWZm4zJmW+a5BaWXAOcBRwGLgPvzQ0cD0yPioI4kNDOzcVux1QUj4mFJvwBeExGDjfslnQycM3R5\nSR60xsxsAiJi8m3viBjxC1gHmJq/Xw34Nelg6vpNy3wU+MEwz43R1l2VL2B26QzO2c18sQrEMlj1\n86Wz1GF7tpoTQhAfhrgP4u1VzFiVr3bVzrH23KcDcyVNIfXnT4uICyWdKmkmEMDtwCGTfpcx644t\ngN/D40+XDtJPIgjgJIn5wI8ldgI+HsFThaPV1qjFPSKuB7Ye5v5/6Fgis86aCSwsHaJfRTBfYmvg\nVOBXEu+O4M7SuerIV6jCYOkALRosHaBFg6UDjKFR3AcL52jVYOkALRpsdcEIHgT2As4GrpR4Q6dC\nDTHYpdephJbPlhn3iqWIdhwUMGsjiV8BR0dwQeksBhK7AD8A5gD/FsEzhSMV167a6eJufUNCpIvw\n/iriuVN5rTCJ9UkF/llg3wjuKxypqHbVTrdlrJ/MAJa6sFdLBPcCuwOXA9dI7Fw4Ui24uFs/8cHU\niorgmQg+BRwM/ETiyPxJyybIxd36iYt7xUVwLrAd8HbgLIm1CkfqWS7u1k9c3HtABH8A/pZ0Dc3V\nEtsUjtSTXNytn7i494gInozgcOBI4FyJD7pNMz4tjy1j1ssk1gbWAm4rncVaF8EZEteSxq96DJhb\nOFLP8J679YtXA9dF8GzpIDY+EdwC7AP8u8T00nl6hYu79Qu3ZHpYBAuA7wDfdHumNS7u1i9c3Hvf\n0cCmwLtKB+kFLu7WL1zce1wETwAHAl+TWLd0nqrz8ANWexKrAH8C1o5gWek8NjkSXwFeGsG+pbN0\ngocfMGvdFsDvXdhr4zPAthJvKR2kylzcrR+4JVMjETwGHEQ6uOorWEfg4m79wMW9ZiL4NXAmcFzp\nLFXl4m79wMW9nj4B7NrFyT56ig+oWq15DPd6k9gdOBnYMoJHSudpBx9QNWvNDDyGe21FMA+YB3yp\ndJaqcXG3unNLpv6OAPaSGCgdpEpc3K3uXNxrLoI/kQr80aWzVImLu9Wdi3t/OAN4mcRrSgepilGL\nu6RVJV0haaGkRZKOyfevLWmepJslnS9panfimo2bi3sfiOBp4CTgsNJZqmLMs2UkrR4Rj0laEbiU\n3N8CHoiIYyV9HFgrIo4a8jyfLWNF5THc7wCmeqjf+ssXNN0GvDKCu0vnmaiunS0TEY/lb1cGViCd\nVrYXzw+aPxd462SDmHWAx3DvIxE8BPwA+GDpLFUwZnGXNEXSQmAJcHFE3ABMi4gleZElwLQOZjSb\nKLdk+s/XgUMkVisdpLQxp9mLiGeBmZJeApwn6XVDHg9Jw/Z2JM1uujkYEYOTyGo2XjOBS0qHsO6J\n4CaJK4F9gTml87RC0gC0/zTOcV2hKunTwDLg/cBARNwraTppj36zIcu6525F5bk3D4rgqtJZrHvy\nVavHA6+KoDOX4HdQV3ruktZpnAkjaTVgd2ABcDZwQF7sAOCsyQYxa6c8hvsmwA2ls1jXXUCqbbuW\nDlLSWG2Z6cBcSVNIG+u0iLhQ0gLgx5IOIp2NsHdnY5qNm8dw71MRhMQJwOHAhaXzlOKBw6yWJA4E\ndotgv9JZrPvyAdX/A3aM4JbSecbDA4eZjc5nyvSx/IntP4GPlM5Siou71ZWLu30T2E+iL6+gd3G3\n2sljuL8auLZ0FisngruA/yFNydd3XNytjmbgMdwt+RpwqDT2NT114+JudeSWjAEQwXzgLuAtpbN0\nm4u71ZGLuzVrnBbZV1zcrY5c3K3ZmcBf9ttY7y7uVkcu7vacPNb7ifTZWO++iMlqRWJd4FZgLQ/1\naw0S6wG3RvDi0lnG4ouYzIa3PTDfhd2GuB+YIlW/uLeLi7vVzQ7A5aVDWLXk0SEXAxuWztItLu5W\nNy7uNhIXd7NeJLECsB1wReksVkku7mY9anNgSQQPlA5ileTibtaj3JKx0bi4m/UoF3cbjYu7WY9y\ncbfRuLib9RqJl5BGg7yucBSrLhd3sx60LbAggqdKB7HK+iOwusQapYN0g4u71YVbMjaqpguZNiid\npRtc3K0uXNytFX3TmnFxt56Xp9VzcbdWuLib9ZBXAMvynJlmo3FxB5C0kaSLJd0g6beSPpLvny1p\nsaQF+WtWd+KaDct77daqvinuY00a+xTw0YhYKGlN4GpJ84AAjo+I4zue0GxsLu7WqsXAHqVDdMOo\ne+4RcW9ELMzfLwV+x/NHmj0Rh1WFi7u1qm/23FvuuUuaAWzF839Eh0q6VtIcSVM7kM1sTBKrA1sA\n15TOYj2hb4r7WG0ZAHJL5gzgsIhYKulbwOfyw0cDxwEHDfO82U03ByNicFJpzZa3NXBDBMtKB7Ge\ncB8wVWLVCB4vHQZA0gAw0Pb1jjWHqqSVgJ8D50bECcM8PgM4JyK2HHK/51C1jpM4AvjLCA4tncV6\ng8QdwK4R3FY6y3C6MoeqJAFzgEXNhV3S9KbF3gZcP9kgZhPkfruNV1+0ZsZqy+wI7AdcJ2lBvu+T\nwD6SZpLOmrkdOKRzEc1GtT3w8dIhrKe4uEfEpQy/d39uZ+KYtU5iQ2BVqObHa6usvijuvkLVetn2\nwOV5QCizVrm4m1Wc++02ES7uZhXn4m4T4eJuVlUSK5EuqruydBbrOS7uZhX2KuD2CB4pHcR6zr3A\nOnkHobZc3K1XuSVjExLB06QrVaePtWwvc3G3XuXibpNR+9aMi7v1Khd3mwwXd7OqkVgHWI80BLXZ\nRLi4m1XQ9sD8CJ4tHcR6lou7WQW5JWOT5eJuVkEu7jZZLu5mVSKxArAdcEXpLNbTXNzNKmYz4L4I\nHigdxHra3cD6eWehllzcrdfsgPfabZIieBJ4EJhWOkunuLhbr9kBuKx0CKuFWrdmXNyt1/hgqrWL\ni7tZFUi8GNgYuK50FqsFF3ezitgWWBDBU6WDWC24uJtVhFsy1k4u7mYV4eJu7eTiblaahHBxt/Zy\ncTergJcDj0dwV+kgVht3ARtI9ayDo/5QkjaSdLGkGyT9VtJH8v1rS5on6WZJ50ua2p241se8125t\nFcEy4FFgndJZOmGsd6yngI9GxCtJf1wfkrQ5cBQwLyI2AS7Mt806ycXdOqG2rZlRi3tE3BsRC/P3\nS0mTI2wA7AXMzYvNBd7ayZBmuLhbZ/RncW8maQawFWlcj2kRsSQ/tIQaj89g5UmsBmwBXFM6i9VO\nbYv7iq0sJGlN4KfAYRHxqKTnHouIkBQjPG92083BiBiceFTrY1sDi3KP1Kydihd3SQPAQLvXO2Zx\nl7QSqbCfFhFn5buXSFo/Iu6VNB24b7jnRsTstiW1fuaWjHXKYmDXkgHyTu9g47akz7ZjvWOdLSNg\nDrAoIk5oeuhs4ID8/QHAWUOfa9ZGHgnSOqX4nnunKGLYjkp6UNoJ+DVpoKbGgp8A5gM/Bl4G3AHs\nHRF/GvLciAhhNkkSdwK7RHBb6SxWLxKbAWdHsEnpLA3tqp2jFvdJrdjF3dpAYkPSgdRpEXTml9X6\nlsSLSCeFrFGV36921c5aXplltbI9cHlV/vCsXiJ4FHgSWKt0lnZzcbeq69jBVEmDkk7sxLr7kaQZ\nkp6VtHUXXusOSf/SptWNq+8uaSD/nGu36fU7wsXdqq6TZ8oEtOcTgaQVJX1Z0rWSlkq6W9L3JW00\nZLlVJJ0o6f683H9L2mDIMmtJOk3Sn/LXqZJeMs48jUL79DAZ1pK0LD++zcR/6uX8AVgfuLaN6xxJ\n2/7vKHxQVdIWeZiXeyUty/d9IZ+p2Fim8YYy9GvEYwUu7lZZEiuRLpy7snSWFqxByvr5/O9bgI2A\nX0paoWm5E4C3A+8BdgZeDPxcUvPf4g+AmcAbgFmk8/xPm2CuxcCBQ+77e1KfuZ0Fkoh4NiLui4hn\n2rXOdpE02mnfpc+YeQL4LrA7PHdg9yDgi8MsuwXpDbTxdetIK3Vxtyp7FXBb7ot2nKTdJD0k6QPj\nfW5EPBwRe0TETyLiloi4EjgE2BzYLK//JcD7gCMi4sKIWADsT/o5X5+X2ZxU1D8QEVdExOV5PW8a\nbS9tFHOB9w657yDge8ALDtpJ+pKkGyU9Jun2/ElklabH50ma13R7TUm3NFpbQ9syTXubsyRdk9f7\na0kbSNpV0nWSHpV0tqS1mta7bR6Q8H5JD0u6RNIO4/mhJc2WdL2k90r6PbBM0uo5yyWSHpT0R0m/\nhHmPk4t708/w9vzz/llp4MTXj/Jaq0g6U9LVksY9CFlE/D4iTo2I6yPiznz3D4Adh1n8/vwG2vh6\ndqT1urhblXXt4iVJ7wR+BhwcEd/J9/19Lj6jfe0zymobrZSH8r/bACsB5zcWiIjFpDGbXpvvei2w\nNCKaz+v/DfDnpmXG43+AVSXtmn+mrUjDJ/94mGWXkvbyNwP+ifTp4l+bHv8HYKakI/LtrwOPA0cw\nutnAoaSD42vl1/4U6U1mAPhroPnCnTVJb0o7kaZWXAj8zwR63Bvnn+EdwKtJe8irA8fn9e4CPAzv\nehc8vtGQ536B9CnrVaRPjj+UtMbQF5D0YuCXwFRgl4h4IN9/wxi/N9ePkf0NNP2eNLkqt/wuULqy\ndUQtDT9gVsgOwMUdfg3lPfVjgXdExAVNj/03Y188NezV2ZJWBo4Dzo6Iu/Pd6wPPRMQfhyy+JD/W\nWOb+5gfzEB/3NS0zHk8Dp5I+MVxEKqg/Ir1ZvEBEfL7p5h8kHQP8C/CZ/Pg9kt4P/Ch/CtkX2DYi\nnhgjw6cj4n8BJH0bOBHYujEooaS5wDubcrzg/1xpqPF3AHsC32/x5wZYGdg/Ipq358+GrPt98Mgj\ncNEW8Mbmh46PiF/kZT5JemN7NemNtmEa8F/AnaRrfZ5semwW6Y18JMvNAyzpN6SWHsAVQ67wvxv4\nR9IbzSqkT3wXStolIi4d7gVc3K3KdgCO6eD6RRrR9BBg54i4ovnBPBLq0nGvNPV3/4vUT39Tizk6\nJYBTgAWSpgH7kKrYcq+ZP70cDryCtPe8AkM+3UfEf0s6nbRH/7GIGGsPFNJFkA2NN8Prh9y3XlOO\n9YCjSXv103KO1UjHMMZj8ZDCjqRX5HVvB6wLTIEQ3LPBkOc2Z74n/7vekGXOI12D8fah7ZGm9sp4\n7E3a7r8Ddpd0bEQcmdd3M3Bz07KXKw3m+DFg2OLutoxVksQ6pD++Gzv4MkE6s+Nu4P3LZxh/WyYX\n9tNJrYbdIuKhpofvBVaQ9BdDXmpafqyxzLpD1ilSYbmXCciF4Rrgh8A9Q9/E8mvskHOfS3pDmklq\nnaw8ZLlVSS2Np4G/ajFC815q5EzPDLmvuRbNJbWwDie1omaSDnq+IEsLlvt0Avwc+AvgA6QCvxXw\nNEwZ+n/yXOZ4/krPofXyHFLraMuhLzKRtkxELI6Ixu/7UcBheuHB+KHmM8r/gffcraq2B+ZHMOIB\noza5jdQPHpT0nYhoPpg6rraM0qlrPySd0TAQEUNbNleTisYepEKKpA1JPe7Gx/3LgDUlvbap7/5a\n0tk4v2Hi5pD24Efqj+8I3BURX2j6eWYMs9y/k9oNewDnSfpFRJwziVwjZTk0Is7NOaYB0ye70vym\nuinwjxHxq3zf1qQ6+KzEi8e5yk+TjqdcKGm3iGg+BXTcbZkhGp+apgAjnX00k7RjMiwXd6uqbhxM\nFWkIjtslvY5U4P8jIg6B8bVl8h7WT4DXAG9Od6nRI/9TRDweEQ9LmgMcm3voD5IO7l0LXJBf83fp\nDA7+Ix8LEPAfwDkRccskftZTSQP+PTzC4zcBG0jal7Td30A6GNn8M+5J2uPdKSKuVBrS+2RJr2qa\n36Edbgb2lzSf1KY4lnQV6WQ9BDwAfEDSXaSJh/4deBqeeJB0xsxj41lhRHwqf7K6IBf46/L9Lbdl\nJO0PLAN+y/M/5xeBH0XEU3mZw4HbgUWkTzD7kU63fftI63VbxqqqG8X9ufO8I+I2Uo93z3zQb7w2\nIs1QNp20h35309feTcsdDpxJOqh5KfAI8Oamj/6QDlReS+rp/hJonDL5HKUrNL87Rqbn1pnPQX9w\nmHZI4/GfkwrdCfm1dyMdSI38euuS9vyPzqd5AnyJ1B8+Zbh1jnB7pGWa73sfqahfTTol8GTSAIXj\nsdw5/Lkv/m7SGTDXkw7sfgp4ApY1ivtImYdbf2O9/wr8J2kPfrkWTQue4vkBGRt7/yeRDn43rET6\n/7mWNJjj3wBvbBqGfTkeOMwqR2IF0l7tyyMYemZJ35O0OmkP9MCI+FHpPHUg8T3g1xEveKMqlMUD\nh1l9bQYscWEf0euAy13Y26r0Vapt5+JuVeSZl0YREb+IiKKzB9WQi7tZF7i4W7e5uJt1gYu7dZuL\nu1kn5XONN+aFVzCadZqLu1mHbQtcEzHmRR5m7fRHYHWJ5QYH61Uu7lY1bslY1+VpHBeTLmyqBRd3\nq5qdgeXGPjHrgsWMf3CyynJxt8qQ2JjUljmvdBbrSzfz/ExIPc/F3arkw8ApEeMfZtesDRaRBn2r\nhTGLu6RTJC1pHqJSaQqrxZIW5K9ZnY1pdSfxItJ0cCcVjmL9q7+KO2ni1qHFO0gzlWyVv37Z/mjW\nZ94LXBTB/5UOYn2rv4p7RFzC83NANvOgYNYWElOAw0gjEpqVchewhsR452qtpMn03A+VdK2kOZKm\nti2R9aO/I+1ATGYyCrNJyadDLgI2L52lHSY6Wce3gM/l748mTQR80NCF8mD+DYMRMTjB17N6Oxw4\nIf9xmZXUaM38b7deUNIAaS6B9q63lfHc83Rb50TEcHMFDvuYx3O3Vki8ijRv58YRbZltx2zCJI4A\nNojgo+UyFBzPXVLzfIZvw+OA2MQdBnzThd0qojYHVcdsy0g6HdgFWEfSncBngQFJM0lnzdwOHNLR\nlFZLEuuR5oAccQZ3sy6rTXH3NHtWjMSngY0i+EDpLGbw3JlbjwAvjeCRMhk8zZ71MIlVgH8CvlY6\ni1lDBM8CN1KDM2Zc3K2UdwPXR3BD6SBmQ9SiNePibl0nIfLpj6WzmA3Dxd1sgnYG1gA8bIVVkYu7\n2QQdDnwt9zfNqqYWxd1ny1hXSbwcmA/M8NC+VkUSKwCPAutG8Ofuv77PlrHe5DHbrdIieIY0ccdm\npbNMxkTHljEbtzxm+wHAVqWzmI2h0Zq5unSQifKeu3XTgaQx2/9QOojZGHq+7+7ibl2R+5gfAb5a\nOotZC1zczVr0d8CDwGWlg5i1wMXdrEUes916ye+BDSVWKx1kolzcrePymO2bAmeUzmLWigieIhX4\nTUpnmSgXd+uGw4BveMx26zE93ZrxqZDWUR6z3XpYTxd377lbpx0C/CSCB0oHMRunni7u3nO3jmka\ns/31pbOYTUBPF3fvuVsn7Y3HbLfedQswQ2Ll0kEmwsXdOiKP2f5RPGa79agIngD+jx49XuTibp2y\nEx6z3Xpfz7ZmXNytUzxmu9WBi7tZg8TGwC7AqaWzmE2Si7tZE4/ZbnVR3+Iu6RRJSyRd33Tf2pLm\nSbpZ0vmSpnY2pvWKPGb7e4GTCkcxa4ebgP8n9d5p463suX8XmDXkvqOAeRGxCXBhvm0GqbBf6DHb\nrQ4iWAbcBbyidJbxGrO4R8QlwEND7t4LmJu/nwu8tc25rAdJTCGNI+PTH61OerI1M9Ge+7SIWJK/\nXwJMa1Me621vxmO2W/30ZHGfdB8pIkLSsGN0S5rddHMwIgYn+3pWTRKrA8cBh3vMdquZRSzfmm4b\nSQPAQNvXGzH236GkGcA5EbFlvn0jMBAR90qaDlwcEZsNeU5EhNod2KpJ4jhgegT7ls5i1k4SrwFO\njmBmd16vPbVzom2Zs0mz2JP/PWuyQax3SewA7EuaI9Wsbm4ENsnzAPeMVk6FPB34DbCppDslHQh8\nCdhd0s3Arvm29SGJVYFTgMM8rK/VUb5e4z5g49JZxmPMnntE7DPCQx7G1QA+Tdqz+UnpIGYd1Dio\nemvpIK3yFao2YRJbAwcDH/JBVKu5njtjxsXdJiSPcf1d4IgI7imdx6zDXNytb3ycdOXeaaWDmHVB\nzxX3lk6FnNCKfSpkbUn8NXAxsFUEi0vnMes0iZcAdwMv6vQw1qVPhbQ+lQdQOgX4pAu79YsIHiYN\nw/Ky0lla5eJu4/VR4BHg5NJBzLqsp1ozLu7WMolNSb32g312jPUhF3ernzzi48nAv0Vwe+k8ZgUs\nArYsHaJVLu7Wqg8BAr5ROohZIRcBs3plGAIXdxtTnhP1s8D7POG19asIbiWdMbNT6SytcHG3UUmI\n1I75cgQ3l85jVtgZwDtLh2iFi7uN5f3Ai4Gvlg5iVgFnAO/Ix6AqrfIBrRyJjYAvktoxT5fOY1Za\nBDcBfwReWzrLWFzcbVi5HfNt4MQIri+dx6xCeqI14+JuI9kP2BCP1W821BnAO6vemql0OCtDYn3S\nfKjvi+DJ0nnMKmYRsBTYtnSQ0bi42wvkdsw3SXNGXl06j1nV5KuzK9+acXG3od4JbA58rnQQswpr\ntGYqO/Kti7s9R2Id4OukdszjpfOYVdh1wNPA1qWDjMTF3ZqdAJwewWWlg5hVWS+0ZjxZh5HHyvgM\nsC/w6ggeKxzJrPIktgF+CGzSzlFSPVmHtYXEesAvgZ2BnV3YzVp2DbAiFR0p0sW9j0nsDFwNXAHs\nHsG9hSOZ9Yyqt2Ym1ZaRdAdpVp5ngKciYrumx9yWqah8hP9jwD8DB0ZwbuFIZj1JYnvguxHtm8Sj\nXbVzxUk+P4CBiHhwskGsOyTWAr4HTAO2i+APZROZ9bT5wJoSW0SwqHSYZu1oy3jvvEfkA0BXA7cD\nf+vCbjY5uTXzUyrYmplscQ/gAklXSTq4HYGs/SQk8UHgXODICA73sAJmbVPJvvtk2zI7RsQ9ktYF\n5km6MSIuaTwoaXbTsoMRMTjJ17NxklgT+A7wSmDHCG4pHMmsbi4D1pHYNA8JPC6SBoCBdodq23nu\nkj4LLI2I4/JtH1AtTOKVpL2K3wAfjmBZ4UhmtSRxInBPBF+c/LoKn+cuaXVJL8rfrwHsAR73uyok\n9gcGSdPjHeTCbtZRlWvNTKYtMw04U1JjPd+PiPPbksomTGJV0vgwuwC7eqINs664FHipxCsi+H3p\nMODhB2pF4hXAT4BbgIMjeKRwJLO+IfEt4PYIjp3cetpTO13ce4DEKqRLnEfbnpuRJtj4HPCNdo51\nYWZjk9gNOCaC7cZceNT1VOMiJuuwpr3xVYE/j7Lon4E3RTC/K8HMbKhfAdMkXh/BBaXDeM+9wiTe\nSjqN8fOkiaq9N25WYRJ7kmYy2zKCpRNbh9sytSWxEnAM8C5g7wiuKBzJzFok8T3gkQg+MrHnu7jX\nksSGwI+APwH/EMEfC0cys3GQWJt0Wvh7IrhkrOWXf77Hc68diT2Aq4CfA292YTfrPRE8CHwImCOx\nWqkc3nOvgKaZkN4P/H0Eg2UTmdlkSfwQ+EMER47veW7LdJzExsC3gQ07/FJrArcB+3jCDLN6kFiX\n1J7Zazxnsbm4d5jEXsDJwJdJ09B1UgA3RfBMh1/HzLpI4j3Ap4BtIniitee4uHeExIrAF4B9gHdH\ncFnhSGbWo/KsZ2cC10Xwmdae4+LedhIvJc1m/hiwXwQPFI5kZj0u15WFwB4RLBx7eZ8t01b50uGr\ngPOBN7qwm1k7RHA3cCRwSr6GpSv6fs9dYgrwr8AHSXvrFxWOZGY1k9sz5wK/HmvM955vy0jMAv66\nIy8+PnuQxm15T36HNTNrO4mXkeYwfkME14y8XI8Wd4mVga8AbwJ+1pEXH5/FwEkRPF06iJnVm8Q7\nSKdX/3MEpw2/TA8Wd4m/BH4M3AMcGMFDHXlxM7OKktiSNHPTIHBYBI+/8PEeO6Aq8XfAfNLwtW9z\nYTezfpRnR9sWmAr8Jg/r3XYdL+4SK0p8kfRR5B0RfMVD15pZP8uzpL0HOAW4TOJt7X6NjrZlIF4K\nnA48RRoz5b6OvJiZWY+S2I7Urv4pcBToyV5oy1xF6ivNcmE3M1teHndma2BTaN+ggZ3ec39DBOd3\n5AXMzGokX3NzJOiYnjtbxszMRlf8bBlJsyTdKOkWSR+fbBAzM2ufCRV3SSsAJwGzgC2AfSRt3s5g\n3SJpoHSGVjhnezlne/VCzl7I2E4T3XPfDrg1Iu6IiKdIIym+pX2xumqgdIAWDZQO0KKB0gFaNFA6\nQIsGSgdo0UDpAC0YKB2gmyZa3DcA7my6vTjfZ2ZmFTDR4u6LkMzMKmxCZ8tI2gGYHRGz8u1PAM9G\nxJeblvEbgJnZBBQ7FVLSisBNwG7A3aQxY/aJiN9NNpCZmU3eihN5UkQ8LenDwHnACsAcF3Yzs+ro\n2EVMZmZWTkfGlumVC5wk3SHpOkkLJM0vnadB0imSlki6vum+tSXNk3SzpPMlTS2ZMWcaLudsSYvz\nNl0gaVbhjBtJuljSDZJ+K+kj+f5Kbc9RclZte64q6QpJCyUtknRMvr9q23OknJXanjnTCjnLOfl2\nW7Zl2/fc8wVONwGvB+4CrqSi/XhJtwPbRMSDpbM0k7QzsBQ4NSK2zPcdCzwQEcfmN8y1IuKoCub8\nLPBoRBxfMluDpPWB9SNioaQ1SdOcvRU4kAptz1Fy7k2FtieApNUj4rF87O1S4AhgLyq0PUfJuRvV\n257/DGwacW84AAACn0lEQVQDvCgi9mrX33on9tx77QKnyo1/ExGXwHKTmewFzM3fzyX94Rc1Qk6o\n0DaNiHsjYmH+finwO9I1GZXanqPkhAptT4CIeCx/uzLpmNtDVGx7wog5oULbU9KGwBuBk3k+V1u2\nZSeKey9d4BTABZKuknRw6TBjmBYRS/L3S4BpJcOM4VBJ10qaU/rjeTNJM4CtgCuo8PZsynl5vqtS\n21PSFEkLSdvt4oi4gQpuzxFyQrW251eBjwHPNt3Xlm3ZieLeS0dod4yIrYA9gQ/lNkPlReqlVXU7\nfwvYGJhJmiv3uLJxktzq+ClwWEQ82vxYlbZnznkGKedSKrg9I+LZiJgJbAj8raTXDXm8EttzmJwD\nVGh7SnoTcF9ELGCETxOT2ZadKO53ARs13d6ItPdeORFxT/73fuBMUkupqpbkviySpkM1Jz+JiPsi\nI33ULL5NJa1EKuynRcRZ+e7Kbc+mnP/VyFnF7dkQEQ8DvyD1iyu3PRuacr6mYtvzb4C98rG/04Fd\nJZ1Gm7ZlJ4r7VcBfSZohaWXg3cDZHXidSZG0uqQX5e/XAPYArh/9WUWdDRyQvz8AOGuUZYvJv4wN\nb6PwNpUkYA6wKCJOaHqoUttzpJwV3J7rNFoZklYDdgcWUL3tOWzORtHMim7PiPhkRGwUERuT5lO9\nKCL2p13bMiLa/kVqc9wE3Ap8ohOv0YaMGwML89dvq5ST9C5+N/Ak6fjFgcDawAXAzcD5wNQK5nwf\ncCpwHXBt/qWcVjjjTqR+5kJSEVpAGqq6UttzhJx7VnB7bglck3NeB3ws31+17TlSzkptz6a8uwBn\nt3Nb+iImM7Ma6vQE2WZmVoCLu5lZDbm4m5nVkIu7mVkNubibmdWQi7uZWQ25uJuZ1ZCLu5lZDf1/\nlNkWoYSCHrwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f0d8790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tt\n",
    "import numpy as np\n",
    "k = 200\n",
    "delta = 1e-2\n",
    "d = 20 \n",
    "n = 2 ** d\n",
    "t = np.linspace(delta, 1, n)\n",
    "t = np.reshape(t, [2] * d, \"F\")\n",
    "t_ten = tt.tensor(t, eps=1e-10)\n",
    "e = tt.ones(2, d)\n",
    "x = tt.kron(t_ten, e)\n",
    "y = tt.kron(e, t_ten)\n",
    "my_fun = lambda x: np.exp(-1j * k * np.sqrt(x[:, 0] ** 2 + x[:, 1] ** 2))/(np.sqrt(x[:, 0] ** 2 + x[:, 1] ** 2))\n",
    "z = tt.multifuncrs([x, y], my_fun, eps=1e-6, verb=False)\n",
    "plt.plot(z.r)\n",
    "plt.text(0.5, 0.5, 'k=%d, Maximal rank=%d' % (k, max(z.r)),  fontsize=14, transform=plt.gca().transAxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ranks in polar  coordinates\n",
    "\n",
    "$$x = r \\cos \\varphi, \\quad y = r \\cos \\varphi, \\quad f(r) = \\frac{\\exp(ikr)}{r}, $$\n",
    "\n",
    "all these functions have bounded QTT-rank!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: active subspaces\n",
    "Another example is to restrict the class of transformations, i.e. linear transformations.  \n",
    "\n",
    "This leads to the approach of **active subspaces**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Active subspaces (cont.)\n",
    "Given a function $f(x_1, \\ldots, x_d)$ we try reparametrize the variable $x$ into a new variable $t$ by a **linear transformation** \n",
    "\n",
    "$$x = W t.$$\n",
    "\n",
    "A natural way to go is to align the axis with the directions where the function **varies the most**:\n",
    "\n",
    "1. Compute the covariance matrix of the gradients $C = E(\\nabla f \\nabla f^{\\top})$\n",
    "2. Compute the  eigenvectors $W$ of  $C_x$\n",
    "\n",
    "This can significantly improve the separability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some examples.\n",
    "1. If the function $f$ is a function of a fewer linear combinations of original variables, then $C_x$ will be not of full rank.\n",
    "2. Gaussian function $\\exp({-x^{\\top} A x})$ is rotated into the product form!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical example from chemistry\n",
    "We have considered the potential energy surface of the water molecule in the Cartesian coordinates.  \n",
    "\n",
    "It has 9 dimensions, and active subspace rotation significantly reduces the ranks.\n",
    "\n",
    "<img width=80% src='cheb_table.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is it possible to learn the network structure?\n",
    "\n",
    "<font size=6.5>\n",
    "The main problems are:\n",
    "- How to select the network structure in the appropriate way?\n",
    "- How to learn the parameters (i.e., tensors living in each vertex) of the network, once you know its structure.\n",
    "- Learn the parameters very well with linear structures (and tensor trains) and also with tree structure (and H-Tucker). \n",
    "\n",
    "** We know particial answers in many cases**, but not in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Missing from this talk\n",
    "- Weighted finite automata $\\approx$ tensor trains with the same cores\n",
    "- Probabilistic context-free grammars "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where to start (literature)\n",
    "- [A literature survey of low‚Äêrank tensor approximation techniques L. Grasedyck, D. Kressner, C. Tobler](http://onlinelibrary.wiley.com/doi/10.1002/gamm.201310004/pdf)\n",
    "- [Tensor decompositions and applications T.G. Kolda, B. W Bader](http://epubs.siam.org/doi/pdf/10.1137/07070111X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Publications and software\n",
    "- http://oseledets.github.io -- Scientific Computing Group at Skoltech\n",
    "- http://github.com/oseledets/TT-Toolbox -- Tensor Trains in MATLAB\n",
    "- http://github.com/oseledets/ttpy -- Tensor Trains in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MMMA-2015\n",
    "In August 23-28 2015 we hold the **4-th Moscow conference in Matrix Methods in Mathematics and Applications**.  \n",
    "\n",
    "Confirmed speakers: C. Schwab, B. Benner, J. White, D. Zorin, M. Benzi, P.-A.Absil, A. Cichocki, P. Van Dooren.\n",
    "\n",
    "**Good time to visit Moscow** (i.e., due to the exchange rate drop from 35 roubles/dollar to 50 roubles/dollar). \n",
    "\n",
    "\n",
    "\n",
    "http://matrix.inm.ras.ru/mmma-2015/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
