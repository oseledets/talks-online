{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical tensor methods in (stochastical) multidimensional problems\n",
    "\n",
    "##### Ivan Oseledets, Skolkovo Institute of Science and Technology \n",
    "##### oseledets.github.io, i.oseledets@skoltech.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skoltech\n",
    "- Founded in 2011 near Moscow http://faculty.skoltech.ru\n",
    "- No departments: priority areas \"IT, Bio, Space and Nuclear\"\n",
    "- At the moment, 160 master students, 30 professors, 40 postdocs, 50 PhD studens \n",
    "- Computational Data-Intensive Science and Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IT  at Skoltech\n",
    "- [Ivan Oseledets](http://faculty.skoltech.ru/Faculty/Ivan-Oseledets): tensors, multidimensional approximations, shape & topology optimization\n",
    "- [Victor Lempitsky](http://faculty.skoltech.ru/Faculty/Victor-Lempitsky): computer vision, deep learning\n",
    "- [Dmitry Vetrov](http://bayesgroup.ru/): machine learning, deep learning\n",
    "- [Panos Karras](http://faculty.skoltech.ru/Faculty/Panagiotis-Karras): data management, data science\n",
    "- [Thanos Polimeridis](http://faculty.skoltech.ru/Faculty/Athanasios-Polimeridis), Maxwell equation, PDEs\n",
    "- [Alexander Shapeev](http://faculty.skoltech.ru/Faculty/Alexander-Shapeev), PDEs, multiscale optimizatio\n",
    "- [Dzmitry-Tsetserukou](http://faculty.skoltech.ru/Faculty/Dzmitry-Tsetserukou), robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Courses\n",
    "- We teach Computational Science & Data Science (NLA, PDEs, Comp Vision, Deep Learning)\n",
    "- We welcome new faculty, postdocs, PhD students, master students\n",
    "- We are open to collaboration with everyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This mini-course\n",
    "This course is about **tensors**. \n",
    "\n",
    "A tensor is just a multidimensional array\n",
    "\n",
    "$$A(i_1, \\ldots, i_d), \\quad 1 \\leq i_k \\leq n_k.$$\n",
    "\n",
    "Suppose that $n_k \\approx n$, \n",
    "the the total amount of memory to be stored is $n^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## These two lectures\n",
    "- Lecture 1: Monday, 17.00 - 19.00. **Introduction**, motivation, matrix background\n",
    "- Lecture 2: Tuesday 17.00 - 19.00. **Novel tensor formats**, advanced algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The core idea\n",
    "Numerical tensor methods have many applications in machine learning, PDEs, data mining.\n",
    "\n",
    "**In one sentence, it is a generalization of standard linear algebra (singular value decomposition)**  \n",
    "\n",
    "to multidimensional case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Connection to stochastics\n",
    "\n",
    "1. Computation of multivariate integrals (over Brownian motion) I. Sloan, H. Woznyakovsky, ...\n",
    "2. PDEs with random coefficients via Karhunen-Loeve decomposition, C. Schwab ..\n",
    "3. Deep learning: connection via quantum information, graphical models, **a new field emerging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Statistics & machine learning\n",
    "Before going into a multidimensional integrals. \n",
    "Now people talk about **machine learning**, but many things **are known**\n",
    "(see table).\n",
    "\n",
    "|Machine learning | \tStatistics |\n",
    "|-----------------|----------------|\n",
    "|network, graphs\tmodel | weights\tparameters |\n",
    "|learning |\tfitting | \n",
    "|generalization\t| test set performance | \n",
    "|supervised learning | \tregression/classiï¬cation | \n",
    "|unsupervised learning\t| density estimation, clustering | \n",
    "|large grant = \\$1,000,000 | large grant = $50,000 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "[Paskov, Traub, Faster valuation of financial derivatives](http://www.iijournals.com/doi/pdfplus/10.3905/jpm.1995.409541)  \n",
    "They considered a **360**-dimensional  collateralized mortgage obligation problems;\n",
    "\n",
    "- The geometric Brownian motion is the canonical model for the interest rate, and the value of the security is an expectation. \n",
    "- For mortgage backed securities the integrand is complicated, due to a prepayment option. The integral cannot be computed analytically. \n",
    "- A typical security of length 360 months yields a 360-dimensional integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte-Carlo\n",
    "A typical way is to use Monte-Carlo sampling: sample $N$ points, compute average\n",
    "- Accuracy is $\\frac{1}{\\sqrt{N}}$, so if you want $10^{-3}$ (absolute) accuracy you need $10^6$ samples.\n",
    "- **Quasi-Monte Carlo** is much more efficient, but still $\\mathcal{O}(\\frac{1}{N})$ in the general case.\n",
    "- Can we get something like $\\exp(-c N^{\\alpha})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor decompositions\n",
    "The idea of tensor decomposition is to represent a tensor (multidimensional array) in terms of simpler array. This idea comes from matrix factorizations (represent a matrix as a product  of simpler ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the simplest tensor decomposition\n",
    "The key idea is the idea of **separation of variables**:\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = U_1(i_1) \\ldots U_d(i_d)$$.\n",
    "\n",
    "In probability theory, it corresponds to the idea of **independence** of variables.\n",
    "\n",
    "This class is too **narrow** to represent useful functions (or probability distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sum-of-products\n",
    "The sum-of-products representation is much more interesting:\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = \\sum_{\\alpha=1}^r U_1(i_1, \\alpha) \\ldots U_d(i_d, \\alpha),$$\n",
    "\n",
    "also named **canonical format**, or **multivariate factor model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two questions\n",
    "- What this decomposition is for $d=2$.\n",
    "- What is good and/or bad for $d \\geq 3$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two-dimensional case\n",
    "For two-dimensional case, it boils down to the approximation\n",
    "$$A \\approx UV^{\\top}, $$\n",
    "where $U$ is $n \\times r$ and $V$ is $m \\times r$.\n",
    "\n",
    "It is **low-rank approximation** of matrices, and best rank-$r$ approximation can be computed  \n",
    "\n",
    "using **singular value decomposition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multidimensional case\n",
    "\n",
    "For $d > 3$\n",
    "The canonical decomposition of the form\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = \\sum_{\\alpha=1}^r U_1(i_1, \\alpha) \\ldots U_d(i_d, \\alpha),$$\n",
    "\n",
    "then it is **very difficult** to compute numerically; there is even an example of $9 \\times 9 \\times 9$ \n",
    "\n",
    "tensor for which the tensor rank is not known!\n",
    "\n",
    "**But this is not the only tensor decomposition** we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Literature on tensor formats\n",
    "- T. Kolda and B. Bader, Tensor decompositions and applications, SIREV (2009)\n",
    "- W. Hackbusch, Tensor spaces and numerical tensor calculus, 2012\n",
    "- L. Grasedyck, D. Kressner, C. Tobler,\n",
    " A literature survey of low-rank tensor approximation techniques, 2013\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Software\n",
    "Some times, it is better to study with the code and to try your own application\n",
    "- Tensor Toolbox 2.5 (T. Kolda)\n",
    "- TT-Toolbox, MATLAB (http://github.com/oseledets/TT-Toolbox)\n",
    "- TT-Toolbox, Python (http://github.com/oseledets/ttpy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic stuff\n",
    "Now we will briefly recall the two-dimensional case, which is important for the techniques for the computational of tensor decompositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Singular value decomposition\n",
    "\n",
    "Given an $n \\times m$ matrix $A$, its **singular value decomposition** (SVD) is defined as \n",
    "\n",
    "$$A = U S V^{\\top},$$\n",
    "where $U^*U = V^* V = I$, and $S$ contains **singular values** on the diagonal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computation of the SVD\n",
    "\n",
    "Standard algorithm for the computation of the SVD requires $\\mathcal{O}(N^3)$ operations,  \n",
    "can be done for matrices $1000-5000$ on a laptop.\n",
    "\n",
    "**Large-scale SVD** is a topic of ongoing research.  \n",
    "\n",
    "The crucial point why it is possible is based on the **skeleton decomposition**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skeleton decomposition\n",
    "\n",
    "Given a rank-$r$ matrix, how many elements you need to compute to recover the full matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skeleton decomposition\n",
    "It is enough to sample $r$ columns and $r$ rows to exactly recover a rank-$r$ matrix:\n",
    "\n",
    "$$A = U \\widehat{A}^{-1} V,$$\n",
    "\n",
    "<img src=\"cross-pic.png\" \\img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Approximate rank-r\n",
    " What happens if the matrix is of approximate low rank?  \n",
    " \n",
    " \n",
    " $A \\approx R + E, \\quad \\mathrm{rank~} R = r, \\quad ||E||_C = \\varepsilon$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum-volume principle\n",
    "\n",
    "Answer is given by the **maximum-volume** principle: select the columns and rows such that  \n",
    "\n",
    "the submatrix $\\widehat{A}$ has maximal-volume among all submatrices, then **quasi-optimality** holds:\n",
    "\n",
    "$$\\Vert A - A_{skel} \\Vert \\leq (r+1)^2 \\Vert A - A_{best} \\Vert.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to earth: applications of maxvol\n",
    "\n",
    "The maximal-volume principle is present in many areas, one of the earliest related references I know is the **Design-optimality** (Fedorov, 1972), and is related to the selection of **optimal interpolation points** for a given basis set:\n",
    "\n",
    "$$f(x) \\approx \\sum_{\\alpha=1}^r c_{\\alpha} g_{\\alpha}(x),$$\n",
    "\n",
    "instead of doing linear regression, just do interpolation at $r$ points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some more ideas\n",
    "- Given a set of stock prices (many!) select the most important ones that we need to track (i.e., to get the insider info...) that will determine the others.\n",
    "- Machine learning tasks: given millions of features, select **most important** features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear algebra showcase\n",
    "\n",
    "Given an $n \\times r$ matrix, find the submatrix of largest **volume** it in.\n",
    "\n",
    "Use greedy algorithm (now we know that it is D-optimality from at least 1972)\n",
    "- Take some rows, put them in the first $r$. Compute $B = A \\widehat{A}^{-1}$\n",
    "- $B = \\begin{pmatrix} I \\\\\n",
    "    Z \\end{pmatrix}$\n",
    "- Suppose maximal element in $Z$ is in position $(i,j)$. \n",
    "- Swap $i$-th row with $j$-th row.\n",
    "- Stop if maximal element is less than $(1+\\delta)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key idea\n",
    "Select subsets of raw data, that describe the behaviour sufficiently well (**interpolation problem**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross approximation\n",
    "For the two-dimensional case, the **best algorithm** to recover the rank-$r$ approximation is  \n",
    "the cross approximation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is it better than random sampling of columns?\n",
    "Randomized techniques for low-rank approximation became popular\n",
    "recently, but no **real comparisons** are done (Martinsson, Halko, Rokhlin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multidimensional case\n",
    "How to generalize the idea of separation of variables to higher\n",
    "dimensions?\n",
    "\n",
    "\n",
    "#### Important points\n",
    "\n",
    "- SVD is good\n",
    "- Best approximation exists\n",
    "- Interpolation via skeleton\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Straightforward idea: canonical format\n",
    "\n",
    "\n",
    "$A(i_1,\\ldots,i_d) \\approx \\sum_{\\alpha=1}^r U_1(i_1,\\alpha)\\ldots\n",
    "U_d(i_d,\\alpha)$  \n",
    "\n",
    "$r$ is called (approximate) canonical rank, $U_k$ --- canonical\n",
    "factors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bad things about the canonical format:\n",
    "- Best approximation may not exist\n",
    "- Canonical rank is NP-complete (matrix rank is ...)\n",
    "- No good algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  ## Bad example (1):\n",
    "  $f(x_1,\\ldots,x_d) = x_1 + x_2 + \\ldots x_d$, \n",
    "  \n",
    "  Canonical rank $d$ (no proof is known), can be\n",
    "  approximated with rank-$2$ with any accuracy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bad example (2):\n",
    "Canonical rank may depend on the field (matrix rank can not!)\n",
    "\n",
    " $f(x_1,\\ldots,x_d) = \\sin(x_1 + \\ldots + x_d)$ \n",
    "- Complex field: 2 (obvious!)\n",
    "- Real field: $d$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another example: Tucker\n",
    "Another attempt to avoid was the Tucker format \n",
    "\n",
    "(Tucker 1966, Lathauwer, 2000+), used first in statistics, where the data was given as a **data cube**.\n",
    "\n",
    "$$A(i,j,k) \\approx \\sum_{\\alpha \\beta \\gamma} G(\\alpha,\\beta,\\gamma)\n",
    "U_1(i,\\alpha)V(j,\\alpha)W(k,\\alpha)$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can compute Tucker by means of the SVD:\n",
    "- Compute <font color='red'> unfoldings </font>: $A_1, A_2, A_3$ \n",
    "- Compute left SVD factors: $A_i \\approx U_i \\Phi_i$\n",
    "- Compute the core: $G = A \\times_1 U^{\\top}_1 \\times_2 U^{\\top}_2\n",
    "  \\times_3 U^{\\top}_3$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main problem with Tucker format\n",
    "\n",
    "The main problem with the Tucker format is that it can not be used to represent **high-dimensional functions**\n",
    "\n",
    "We reduce $n^d$ complexity to $r^d$, which makes $d = 10,\\ldots, 20$ tracktable, but not more.\n",
    "\n",
    "Are there other ideas?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main motivation\n",
    "Matrices are good, so reduce everything to matrices! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Splitting into halves\n",
    "Slit index set into two, get $n^{d/2} \\times n^{d/2}$ matrix, compute its SVD:\n",
    "\n",
    "$$A(I, J) = \\sum_{\\alpha=1}^r U_{I, \\alpha} V_{J, \\alpha},$$\n",
    "\n",
    "then we have $d_1 +1 $ and $d_2 + 1$ - dimensional tensors. Do it recursively!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another construction\n",
    "Another construction, so-called Hierarchical Tucker, forms the construction:\n",
    "\n",
    "1. Compute Tucker decomposition, get a $r \\times r \\times \\ldots \\times r$ array;\n",
    "2. Merge indices by two, and compute **transfer matrices** of size $r \\times r \\times r$.\n",
    "3. Go up the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplest construction\n",
    "The simplest construction **separates** one index at a time and gives the **tensor-train** (or **matrix-product state**) decomposition\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) \\approx G_1(i_1) \\ldots G_d(i_d), $$\n",
    "\n",
    "where $G_k(i_k)$ is $r_{k-1} \\times r_k$ matrix for any fixed $r_k$.\n",
    "\n",
    "**Similar to Hidden Markov models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "- Tensors are a natural model for multivariate probability distributions\n",
    "- Can be used to compute expectations, quantiles, solve stochastic PDEs\n",
    "- Tensor decompositions are needed to reduce the curse of dimensionality\n",
    "- Goal: is to beat Monte-Carlo by using **low-rank** structure\n",
    "- In order to build tensor decompositions, we need to know about matrix methods\n",
    "- Standard tensor decomposition are difficult to be used numerically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture (tomorrow)\n",
    "- Basic properties of the tensor trains\n",
    "- Basic problems & algorithms\n",
    "- Application to graphical models\n",
    "- Application to path integrals (integration over Brownian motion)\n",
    "- Application to financial integrals\n",
    "- Application to machine learning problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
