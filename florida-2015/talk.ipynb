{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensor methods: A tool for high-dimensional problems\n",
    "\n",
    "##### Ivan Oseledets, Skolkovo Institute of Science and Technology \n",
    "##### oseledets.github.io, i.oseledets@skoltech.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skoltech\n",
    "- Founded in 2011 near Moscow http://faculty.skoltech.ru\n",
    "- No departments: priority areas \"IT, Bio, Space and Nuclear\"\n",
    "- At the moment, 160 master students, 30 professors, 40 postdocs, 50 PhD studens \n",
    "- I work at Center for Computational Data-Intensive Science and Engineering http://crei.skoltech.ru/cdise/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CDISE  at Skoltech\n",
    "- [Ivan Oseledets](http://faculty.skoltech.ru/Faculty/Ivan-Oseledets): tensors, multidimensional approximations, shape & topology optimization\n",
    "- [Victor Lempitsky](http://faculty.skoltech.ru/Faculty/Victor-Lempitsky): computer vision, deep learning\n",
    "- [Dmitry Vetrov](http://bayesgroup.ru/): machine learning, deep learning\n",
    "- [Panos Karras](http://faculty.skoltech.ru/Faculty/Panagiotis-Karras): data management, data science\n",
    "- [Thanos Polimeridis](http://faculty.skoltech.ru/Faculty/Athanasios-Polimeridis), Maxwell equation, PDEs\n",
    "- [Alexander Shapeev](http://faculty.skoltech.ru/Faculty/Alexander-Shapeev), PDEs, multiscale optimizatio\n",
    "- [Dzmitry-Tsetserukou](http://faculty.skoltech.ru/Faculty/Dzmitry-Tsetserukou), robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Courses\n",
    "- We teach Computational Science & Data Science (NLA, PDEs, Comp Vision, Deep Learning)\n",
    "- We welcome new faculty, postdocs, PhD students, master students\n",
    "- We are open to collaboration with everyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This talk\n",
    "This talk is about **tensors**. \n",
    "\n",
    "A tensor is just a multidimensional array\n",
    "\n",
    "$$A(i_1, \\ldots, i_d), \\quad 1 \\leq i_k \\leq n_k.$$\n",
    "\n",
    "Suppose that $n_k \\approx n$, \n",
    "the the total amount of memory to be stored is $n^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The core idea\n",
    "Numerical tensor methods have many applications in machine learning, PDEs, data mining.\n",
    "\n",
    "**In one sentence, it is a generalization of standard linear algebra (singular value decomposition)**  \n",
    "\n",
    "to multidimensional case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Connection to stochastics/risk management/etc...\n",
    "\n",
    "1. Computation of multivariate integrals (over Brownian motion) I. Sloan, H. Woznyakovsky, ...\n",
    "2. PDEs with random coefficients via Karhunen-Loeve decomposition, C. Schwab ..\n",
    "3. Deep learning: connection via quantum information, graphical models, **a new field emerging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Statistics & machine learning\n",
    "Before going into a multidimensional integrals. \n",
    "Now people talk about **machine learning**, but many things **are known**\n",
    "(see table).\n",
    "\n",
    "|Machine learning | \tStatistics |\n",
    "|-----------------|----------------|\n",
    "|network, graphs\tmodel | weights\tparameters |\n",
    "|learning |\tfitting | \n",
    "|generalization\t| test set performance | \n",
    "|supervised learning | \tregression/classiï¬cation | \n",
    "|unsupervised learning\t| density estimation, clustering | \n",
    "|large grant = \\$1,000,000 | large grant = $50,000 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "[Paskov, Traub, Faster valuation of financial derivatives](http://www.iijournals.com/doi/pdfplus/10.3905/jpm.1995.409541)  \n",
    "They considered a **360**-dimensional  collateralized mortgage obligation problems;\n",
    "\n",
    "- The geometric Brownian motion is the canonical model for the interest rate, and the value of the security is an expectation. \n",
    "- For mortgage backed securities the integrand is complicated, due to a prepayment option. The integral cannot be computed analytically. \n",
    "- A typical security of length 360 months yields a 360-dimensional integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte-Carlo\n",
    "A typical way is to use Monte-Carlo sampling: sample $N$ points, compute average\n",
    "- Accuracy is $\\frac{1}{\\sqrt{N}}$, so if you want $10^{-3}$ (absolute) accuracy you need $10^6$ samples.\n",
    "- **Quasi-Monte Carlo** is much more efficient, but still $\\mathcal{O}(\\frac{1}{N})$ in the general case.\n",
    "- Can we get something like $\\exp(-c N^{\\alpha})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor decompositions\n",
    "The idea of tensor decomposition is to represent a tensor (multidimensional array) in terms of simpler array. This idea comes from matrix factorizations (represent a matrix as a product  of simpler ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the simplest tensor decomposition\n",
    "The key idea is the idea of **separation of variables**:\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = U_1(i_1) \\ldots U_d(i_d)$$.\n",
    "\n",
    "In probability theory, it corresponds to the idea of **independence** of variables.\n",
    "\n",
    "This class is too **narrow** to represent useful functions (or probability distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sum-of-products\n",
    "The sum-of-products representation is much more interesting:\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = \\sum_{\\alpha=1}^r U_1(i_1, \\alpha) \\ldots U_d(i_d, \\alpha),$$\n",
    "\n",
    "also named **canonical format**, or **multivariate factor model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two questions\n",
    "- What this decomposition is for $d=2$.\n",
    "- What is good and/or bad for $d \\geq 3$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two-dimensional case\n",
    "For two-dimensional case, it boils down to the approximation\n",
    "$$A \\approx UV^{\\top}, $$\n",
    "where $U$ is $n \\times r$ and $V$ is $m \\times r$.\n",
    "\n",
    "It is **low-rank approximation** of matrices, and best rank-$r$ approximation can be computed  \n",
    "\n",
    "using **singular value decomposition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multidimensional case\n",
    "\n",
    "For $d > 3$\n",
    "The canonical decomposition of the form\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = \\sum_{\\alpha=1}^r U_1(i_1, \\alpha) \\ldots U_d(i_d, \\alpha),$$\n",
    "\n",
    "then it is **very difficult** to compute numerically; there is even an example of $9 \\times 9 \\times 9$ \n",
    "\n",
    "tensor for which the tensor rank is not known!\n",
    "\n",
    "**But this is not the only tensor decomposition** we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Literature on tensor formats\n",
    "- T. Kolda and B. Bader, Tensor decompositions and applications, SIREV (2009)\n",
    "- W. Hackbusch, Tensor spaces and numerical tensor calculus, 2012\n",
    "- L. Grasedyck, D. Kressner, C. Tobler,\n",
    " A literature survey of low-rank tensor approximation techniques, 2013\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Software\n",
    "Some times, it is better to study with the code and to try your own application\n",
    "- Tensor Toolbox 2.5 (T. Kolda)\n",
    "- TT-Toolbox, MATLAB (http://github.com/oseledets/TT-Toolbox)\n",
    "- TT-Toolbox, Python (http://github.com/oseledets/ttpy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic stuff\n",
    "Now we will briefly recall the two-dimensional case, which is important for the techniques for the computational of tensor decompositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Singular value decomposition\n",
    "\n",
    "Given an $n \\times m$ matrix $A$, its **singular value decomposition** (SVD) is defined as \n",
    "\n",
    "$$A = U S V^{\\top},$$\n",
    "where $U^*U = V^* V = I$, and $S$ contains **singular values** on the diagonal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computation of the SVD\n",
    "\n",
    "Standard algorithm for the computation of the SVD requires $\\mathcal{O}(N^3)$ operations,  \n",
    "can be done for matrices $1000-5000$ on a laptop.\n",
    "\n",
    "**Large-scale SVD** is a topic of ongoing research.  \n",
    "\n",
    "The crucial point why it is possible is based on the **skeleton decomposition**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skeleton decomposition\n",
    "\n",
    "Given a rank-$r$ matrix, how many elements you need to compute to recover the full matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skeleton decomposition\n",
    "It is enough to sample $r$ columns and $r$ rows to exactly recover a rank-$r$ matrix:\n",
    "\n",
    "$$A = U \\widehat{A}^{-1} V,$$\n",
    "\n",
    "<img src=\"cross-pic.png\" \\img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Approximate rank-r\n",
    " What happens if the matrix is of approximate low rank?  \n",
    " \n",
    " \n",
    " $A \\approx R + E, \\quad \\mathrm{rank~} R = r, \\quad ||E||_C = \\varepsilon$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum-volume principle\n",
    "\n",
    "Answer is given by the **maximum-volume** principle: select the columns and rows such that  \n",
    "\n",
    "the submatrix $\\widehat{A}$ has maximal-volume among all submatrices, then **quasi-optimality** holds:\n",
    "\n",
    "$$\\Vert A - A_{skel} \\Vert \\leq (r+1)^2 \\Vert A - A_{best} \\Vert.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to earth: applications of maxvol\n",
    "\n",
    "The maximal-volume principle is present in many areas, one of the earliest related references I know is the **Design-optimality** (Fedorov, 1972), and is related to the selection of **optimal interpolation points** for a given basis set:\n",
    "\n",
    "$$f(x) \\approx \\sum_{\\alpha=1}^r c_{\\alpha} g_{\\alpha}(x),$$\n",
    "\n",
    "instead of doing linear regression, just do interpolation at $r$ points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some more ideas\n",
    "- Given a set of stock prices (many!) select the most important ones that we need to track (i.e., to get the insider info...) that will determine the others.\n",
    "- Machine learning tasks: given millions of features, select **most important** features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear algebra showcase\n",
    "\n",
    "Given an $n \\times r$ matrix, find the submatrix of largest **volume** it in.\n",
    "\n",
    "Use greedy algorithm (now we know that it is D-optimality from at least 1972)\n",
    "- Take some rows, put them in the first $r$. Compute $B = A \\widehat{A}^{-1}$\n",
    "- $B = \\begin{pmatrix} I \\\\\n",
    "    Z \\end{pmatrix}$\n",
    "- Suppose maximal element in $Z$ is in position $(i,j)$. \n",
    "- Swap $i$-th row with $j$-th row.\n",
    "- Stop if maximal element is less than $(1+\\delta)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key idea\n",
    "Select subsets of raw data, that describe the behaviour sufficiently well (**interpolation problem**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross approximation\n",
    "For the two-dimensional case, the **best algorithm** to recover the rank-$r$ approximation is  \n",
    "the cross approximation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is it better than random sampling of columns?\n",
    "Randomized techniques for low-rank approximation became popular\n",
    "recently, but no **real comparisons** are done (Martinsson, Halko, Rokhlin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multidimensional case\n",
    "How to generalize the idea of separation of variables to higher\n",
    "dimensions?\n",
    "\n",
    "\n",
    "#### Important points\n",
    "\n",
    "- SVD is good\n",
    "- Best approximation exists\n",
    "- Interpolation via skeleton\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Straightforward idea: canonical format\n",
    "\n",
    "\n",
    "$A(i_1,\\ldots,i_d) \\approx \\sum_{\\alpha=1}^r U_1(i_1,\\alpha)\\ldots\n",
    "U_d(i_d,\\alpha)$  \n",
    "\n",
    "$r$ is called (approximate) canonical rank, $U_k$ --- canonical\n",
    "factors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bad things about the canonical format:\n",
    "- Best approximation may not exist\n",
    "- Canonical rank is NP-complete (matrix rank is ...)\n",
    "- No good algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  ## Bad example (1):\n",
    "  $f(x_1,\\ldots,x_d) = x_1 + x_2 + \\ldots x_d$, \n",
    "  \n",
    "  Canonical rank $d$ (no proof is known), can be\n",
    "  approximated with rank-$2$ with any accuracy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bad example (2):\n",
    "Canonical rank may depend on the field (matrix rank can not!)\n",
    "\n",
    " $f(x_1,\\ldots,x_d) = \\sin(x_1 + \\ldots + x_d)$ \n",
    "- Complex field: 2 (obvious!)\n",
    "- Real field: $d$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another example: Tucker\n",
    "Another attempt to avoid was the Tucker format \n",
    "\n",
    "(Tucker 1966, Lathauwer, 2000+), used first in statistics, where the data was given as a **data cube**.\n",
    "\n",
    "$$A(i,j,k) \\approx \\sum_{\\alpha \\beta \\gamma} G(\\alpha,\\beta,\\gamma)\n",
    "U_1(i,\\alpha)V(j,\\alpha)W(k,\\alpha)$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can compute Tucker by means of the SVD:\n",
    "- Compute <font color='red'> unfoldings </font>: $A_1, A_2, A_3$ \n",
    "- Compute left SVD factors: $A_i \\approx U_i \\Phi_i$\n",
    "- Compute the core: $G = A \\times_1 U^{\\top}_1 \\times_2 U^{\\top}_2\n",
    "  \\times_3 U^{\\top}_3$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main problem with Tucker format\n",
    "\n",
    "The main problem with the Tucker format is that it can not be used to represent **high-dimensional functions**\n",
    "\n",
    "We reduce $n^d$ complexity to $r^d$, which makes $d = 10,\\ldots, 20$ tracktable, but not more.\n",
    "\n",
    "Are there other ideas?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main motivation\n",
    "Matrices are good, so reduce everything to matrices! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Splitting into halves\n",
    "Slit index set into two, get $n^{d/2} \\times n^{d/2}$ matrix, compute its SVD:\n",
    "\n",
    "$$A(I, J) = \\sum_{\\alpha=1}^r U_{I, \\alpha} V_{J, \\alpha},$$\n",
    "\n",
    "then we have $d_1 +1 $ and $d_2 + 1$ - dimensional tensors. Do it recursively!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another construction\n",
    "Another construction, so-called Hierarchical Tucker, forms the construction:\n",
    "\n",
    "1. Compute Tucker decomposition, get a $r \\times r \\times \\ldots \\times r$ array;\n",
    "2. Merge indices by two, and compute **transfer matrices** of size $r \\times r \\times r$.\n",
    "3. Go up the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplest construction\n",
    "The simplest construction **separates** one index at a time and gives the **tensor-train** (or **matrix-product state**) decomposition\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) \\approx G_1(i_1) \\ldots G_d(i_d), $$\n",
    "\n",
    "where $G_k(i_k)$ is $r_{k-1} \\times r_k$ matrix for any fixed $r_k$.\n",
    "\n",
    "**Similar to Hidden Markov models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor-train\n",
    "Tensor-train (in physics known as matrix-product-state) representation has the form\n",
    "\n",
    "$$\n",
    "   A(i_1, \\ldots, i_d) = G_1(i_1) \\ldots G_d(i_d), \n",
    "$$\n",
    "where $G_k(i_k)$ has size $r_{k-1} \\times r_k$ and $r_0 = r_d = 1$. \n",
    "\n",
    "Now we have **d-1** ranks (we call them TT-ranks). The parametrization is defined by **TT-cores**.\n",
    "\n",
    "Why tensor train? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TT-ranks are matrix ranks\n",
    " Define unfoldings: \n",
    " \n",
    " $A_k = A(i_1 \\ldots i_k; i_{k+1} \\ldots i_d)$, $n^k \\times n^{d-k}$\n",
    " matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   Theorem: there exists a TT-decomposition with TT-ranks  \n",
    "   \n",
    "   $$r_k = \\mathrm{rank} A_k$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The proof \n",
    "Proof is simple and give the TT-SVD algorithm (in quantum information it is known as Vidal algorithm).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stability estimate\n",
    "No exact ranks in practice -- stability estimate! \n",
    "\n",
    "If $A_k = R_k + E_k$, $||E_k|| = \\varepsilon_k$\n",
    "  $$||\\mathbf{A}-\\mathbf{TT}||_F \\leq \\sqrt{\\sum_{k=1}^{d-1} \\varepsilon^2_k}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fast and trivial linear algebra\n",
    " Addition, Hadamard product, scalar product, convolution \n",
    " \n",
    " All scale linear in $d$ \n",
    " \n",
    " $$C(i_1,\\ldots,i_d) = A(i_1,\\ldots,i_d) B(i_1,\\ldots,i_d)$$\n",
    "    \n",
    "   $$C_k(i_k) = A_k(i_k) \\otimes B_k(i_k),$$\n",
    "   ranks are multiplied\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rounding\n",
    "**Rounding** procedure solves the following problem: given a TT-representation\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = G_1(i_1) G_2(i_2) \\ldots G_d(i_d)$$\n",
    "\n",
    "find **another one** with smaller memory that approximates it with **threshold** $\\epsilon$.\n",
    "\n",
    "It can be done in $\\mathcal{O}(dnr^3)$ operations using the **linear structure** of the format:\n",
    "\n",
    "We can do orthogonalization of the cores! (whiteboard again)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross approximation in $d$-dimensions\n",
    "\n",
    "Oseledets, Tyrtyshnikov, TT-cross approximation of multidimensional arrays, which generalizes the matrix result.\n",
    "\n",
    "**Theorem:** A rank-$r$ tensor can be recovered from $\\mathcal{O}(dnr^2)$ elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantized tensor-train\n",
    "To used tensor decompositions, we have to **approximate** a continious problem by a discrete, where the **vector of unknowns** can be indexed by a $d$-index.\n",
    "\n",
    "1. (Standard) Given a function of $d$ variables, introduce a tensor-product basis set, and the tensor is the tensor of coefficients\n",
    "2. (Non-standard) Given a function of one variable, take it on a uniform grid with $2^d$ points, reshape into a $2 \\times \\ldots \\times 2$ $d$-dimensional tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantized tensor train\n",
    "\n",
    "It is a mapping from 1D functions to $d$-dimensional arrays, using the following representation:\n",
    "\n",
    "$$v_k = f(x_k), \\quad k = 1, \\ldots, 2^d.$$\n",
    "\n",
    "$V(i_1, \\ldots, i_d) = v(a + i h)$, $\\quad i = i_1 + 2 i_2 + \\ldots 2^{d-1} i_d.$\n",
    "\n",
    "It gives $\\mathcal{O}(2dr^2) = \\mathcal{O}(\\log N)$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation of some basic functions\n",
    "Almost all important functions in 1D are of low QTT-ranks\n",
    "\n",
    "- $f(x) = \\exp(\\lambda x)$ -- rank $1$\n",
    "- $f(x) = \\sin( w x + \\alpha)$ -- rank $2$\n",
    "- $f(x)$ -- polynomial, rank $p+1$\n",
    "- Rational functions, even non-smooth functions (Weirstrass function has bounded QTT-ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical uncertainty quantification\n",
    "\n",
    "One of the examples:\n",
    "\n",
    "- We have a (hierarchical) model with random input. \n",
    "\n",
    "- Each output becomes a random variable as well.\n",
    "\n",
    "http://arxiv.org/pdf/1407.3023.pdf Enabling High-Dimensional Hierarchical\n",
    "Uncertainty Quantification by ANOVA and\n",
    "Tensor-Train Decomposition, Zheng Zhang, Xiu Yang, Ivan V. Oseledets, George Em Karniadakis, and Luca Daniel\n",
    "\n",
    "For the stochastic circuit simulator it gives over **200x** speedup for a **45-parameter** system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Path integrals\n",
    "Integral over Brownian motion the one-dimensional reaction-diffusion equation\n",
    "with initial distribution $f(x): \\mathbb{R} \\to \\mathbb{R}^{+}$\n",
    "and a constant diffusion coefficient~$\\sigma$\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    " & \\frac{\\partial}{\\partial t} u(x,t)\n",
    " = \\sigma \\frac{\\partial^2}{\\partial x^2} u(x,t) - V(x,t) u(x,t),\\\\\n",
    "& u(x,0)=f(x)\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\qquad t \\in [0, T],\n",
    "\\quad x \\in \\mathbb{R}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feyman-Kac formula\n",
    "The solution can be expressed by the Feynman-Kac\n",
    "formula\n",
    "\\begin{equation}\n",
    "u_{f}(x,T)=\\int_{\\mathcal C\\{x,0; T \\}}  f(\\xi(T))\n",
    " e^{-\\int_{0}^{T}\\! V(\\xi(\\tau),T-\\tau) d\\tau }\n",
    "\\mathcal{D}_{\\xi},\n",
    "\\end{equation}\n",
    "where the integration is done over a set of all\n",
    "continuous paths $\\xi(T): [0,T]\\to \\mathbb{R}$\n",
    "from the Banach space $\\Xi([0,T], \\mathbb{R})$\n",
    "starting at $\\xi(0)=x$ and stopping at arbitrary endpoints at time $T$.\n",
    "$\\mathcal{D}_{\\xi}$ is the Wiener measure,\n",
    "and $\\xi(t)$ is the Wiener process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discretization\n",
    "A standard way to discretize the path integral is\n",
    "to break the time range $[0,T]$ into $n$ intervals\n",
    "\\begin{equation}\n",
    "\\tau_k=k \\cdot \\delta t, \\qquad 0\\le k\\le n,\n",
    "\\qquad n\\!: \\tau_n=T.\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "u^{(n)}(x,t)=\\int_{-\\infty}^{\\infty} \\mathcal{D}^{(n)}_{\\xi}\n",
    "\\, f\\!\\left( \\xi^{(n)}\\right) \\! \\prod_{i=0}^{n}\n",
    "e^{ -w_{i} V^{(n)}_{i} \\delta t },\n",
    "\\end{equation}\n",
    "The integration sign here denotes an $n$-dimensional integration\n",
    "over the particle steps $\\xi_k$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numerical experiment\n",
    "For numerical experiment we used diffusion equation in a weird potential which is non-periodic.\n",
    "<p>\n",
    "</p>\n",
    "<div style=\"float: left; width: 40%; margin-right: 5%; margin-bottom: 0.5em\">\n",
    "Solution (and convergence)\n",
    "<img src=\"ux2.jpg\" width=100%> \n",
    "</div>\n",
    "<div style=\"float: left; width: 40%; margin-right: 5%; margin-bottom: 0.5em\">\n",
    "Potential and initial condition\n",
    "<img src=\"vf1.jpg\" width=100%>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Financial integral\n",
    "<font size=4.0>\n",
    "We compute PV (Present Discounted Value) that takes into account that value of money changes in time due to basic economic parameters.\n",
    "\\begin{equation}\n",
    "PV=\\left( \\frac{\\alpha}{\\pi} \\right)^{d/2} \\int_{-\\infty}^{\\infty} \\ldots \\int_{-\\infty}^{\\infty} v(\\xi_1,\\ldots, \\xi_d)  \n",
    "\\, e^{-\\alpha \\xi_{1}^2}\\ldots e^{-\\alpha \\xi_{d}^2}\n",
    "   d\\xi_d \\ldots d\\xi_1\n",
    "\\qquad\n",
    " v(\\xi_1,\\ldots, \\xi_d) = \\sum_{k=1}^{d} u_k m_k\n",
    "\\end{equation}\n",
    "\\begin{equation*}\n",
    "u_k=\\prod_{j=0}^{k-1} \\frac{1}{1+i_j},\n",
    "\\qquad\n",
    "m_k=cr_k(1-w_k +w_k c_k),\n",
    "\\qquad\n",
    "r_k=\\prod_{j=1}^{k-1}(1-w_j),\n",
    "\\end{equation*}\n",
    "\\begin{equation}\n",
    "c_k = \\sum^{d-k}_{j=0}\\frac{1}{(1+i_0)^j}\n",
    "\\qquad\n",
    "w_k=K_1 + K_2 \\arctan(K_3 i_k + K_4),\n",
    "\\qquad\n",
    "i_k = K^{k}_0 e^{\\xi_1 + \\ldots + \\xi_k} i_0\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "Accuracy of the cross approximation $\\varepsilon = 10^{-10}$.\n",
    "Dimension of the integral is labeled by $n$,\n",
    "It corresponds to the number of monthes in mortgage-backed security model.\n",
    "Its parameters are\n",
    "$(i_0,K_1,K_2,K_3, K_4, \\sigma^2)=(0.007,0.01, -0.005, 10, 0.5,0.0004)$.\n",
    "Ranks of the matrix are presented in column labeled by $r$.\n",
    "\n",
    " |n | r |  $T_{cross}$ (min.) |  $T_{MC}$ (min.)| \n",
    " |--|-|-----------------------|-----------------|\n",
    " |36|   7    |0.1|        7.7   | \n",
    "     |48|   10| 0.16|     11.6  | \n",
    "     |90|   10| 0.3|  21.4| \n",
    "     |180|   10| 0.5| 41.5 | \n",
    "     |360|  10| 0.8|  81.6 | \n",
    "     |720|  9| 1.56|  178| \n",
    "   |1440|  9| 2.81|  354| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What I did not talk about\n",
    "- PDEs with random coefficients (use KL-decomposition, obtain a multivariate function, approximate in the TT-format)\n",
    "- Connection to other high-dimensional tools, including machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "- Tensors are a natural model for multivariate probability distributions\n",
    "- Can be used to compute expectations, quantiles, solve stochastic PDEs\n",
    "- Tensor decompositions are needed to reduce the curse of dimensionality\n",
    "- Goal: is to beat Monte-Carlo by using **low-rank** structure\n",
    "- In order to build tensor decompositions, we need to know about matrix methods\n",
    "- Standard tensor decomposition are difficult to be used numerically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
