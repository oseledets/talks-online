{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scroll': True,\n",
       " 'start_slideshow_at': 'selected',\n",
       " 'theme': 'sky',\n",
       " 'transition': 'zoom'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "#path = \"/home/damian/miniconda3/envs/rise_latest/etc/jupyter/nbconfig\"\n",
    "cm = BaseJSONConfigManager()\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'sky',\n",
    "              'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "              'scroll': True\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensor decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overall plan\n",
    "1. **Multivariate function approximation:** curse of dimensionality, polynomial chaos, optimal experiment design, connection to linear algebra.\n",
    "\n",
    "2. **Tensor decompositions:**\n",
    "\n",
    "3. **Deep learning methods.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward problem\n",
    "\n",
    "In the forward propagation of uncertainty, we have a known model F for a system of interest. \n",
    "\n",
    "We model its inputs $X$ as a random variable and wish to understand the output random variable \n",
    "\n",
    "$$Y = F(X)$$\n",
    "\n",
    "(also denoted $Y \\vert X$) and reads $Y$ given $X$. \n",
    "\n",
    "We want to infer statistical properties of $Y$ given statistical properties of $X$. \n",
    "\n",
    "In the simplest case, this correponds to the computation of **high-dimensional integrals.**\n",
    "\n",
    "$$E Y = \\int F(X) \\rho(X) dX.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximation\n",
    "\n",
    "In the simplest case, this correponds to the computation of **high-dimensional integrals.**\n",
    "\n",
    "$$E Y = \\int F(X) \\rho(X) dX.$$\n",
    "\n",
    "We select the model $G(X, \\theta)$ and fit $\\theta$ such that\n",
    "\n",
    "$$F(X_i) \\approx G(X_i, \\theta), \\quad i =1, \\ldots, N_s.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate function interpolation\n",
    "There are **not too many** efficient methods for working with multivariate functions:\n",
    "\n",
    "- Radial basis functions\n",
    "- Sparse grids\n",
    "- Best $N$-term approximation (wavelets?)\n",
    "- Different machine learning techniques (decision trees, neural networks)\n",
    "\n",
    "Many successful methods are based on the idea of **separation of variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Separation of variables\n",
    "Function is called **separable**, if \n",
    "$$ f(x_1, \\ldots, x_d) = f_1(x_1) f_2(x_2) \\ldots f_d(x_d).$$\n",
    "\n",
    "Not many functions can be represented in this form  \n",
    "\n",
    "(or even approximated, it means **independence** of variables in some sense.)\n",
    "\n",
    "A more general and efficient class of functions is the **sum of separable functions** (known also proper generalized decomposition).\n",
    "\n",
    "$$f(x_1,\\ldots, x_d) \\approx \\sum_{\\alpha=1}^r f^{(\\alpha)}_1(x_1) \\ldots f^{(\\alpha)}_d(x_d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting by sums of separable functions\n",
    "How to fit a given function $f$ by a sums of separable functions?  \n",
    "\n",
    "We can always consider $x_1, \\ldots, x_d$ to be discrete, i.e\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = f(x_1(i_1), \\ldots, x_d(i_d))$$  \n",
    "\n",
    "Second, we approximate the tensor by **sampling**.  \n",
    "\n",
    "Why it is possible to be done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Number of parameters\n",
    "In the discrete case, representation\n",
    "$$A(i_1, \\ldots, i_d) \\approx \\sum_{\\alpha=1}^r U_1(i_1, \\alpha) \\ldots U_d(i_d, \\alpha)$$\n",
    "is called **canonical format**, or canonical-polyadic (CP-format). \n",
    "\n",
    "The  number of parameters is $dnr$, so if $r$ is small, a good idea is that it is possible to fit those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix case\n",
    "In two dimensions separation boils down to **low-rank approximation**:\n",
    "$$A \\approx UV^{\\top}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skeleton decomposition\n",
    "How can we approximation a low-rank matrix without computing all its elements?  \n",
    "The skeleton decomposition gives an answer: if a matrix has rank $r$, it can be exactly recovered from its $r$ linearly independent columns and $r$ linearly independent rows.  \n",
    "<img src =\"cross-pic.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross approximation\n",
    "Cross approximation is a heuristic sampling technique for the selection of \"good\" rows and columns in a low-rank matrix.  \n",
    "\n",
    "The goal is to find rows and column that **maximize the volume** of the submatrix.  \n",
    "\n",
    "The maximal volume principle (Goreinov, Tyrtyshnikov) states that\n",
    "\n",
    "$$  \\Vert A - A_{skel} \\Vert_C \\leq (r + 1) \\sigma_{r+1},$$\n",
    "\n",
    "and $\\Vert \\cdot \\Vert$ is the maximal in modulus element of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate case: problems\n",
    "\n",
    "There is a big problem in many dimensions:  \n",
    "\n",
    "it is not possible to do the exact interpolation trick for the **sum-of-products** (canonical format).  \n",
    "\n",
    "The problem is not with the algorithms but with the **format** itself:  \n",
    "\n",
    "the best approximation may not even exist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bad example\n",
    "$$f(x_1, \\ldots, x_d) = x_1 + \\ldots + x_d$$.\n",
    "\n",
    "$$g(t) = (1 + x_1 t) (1 + x_2 t) \\ldots (1 + x_d t)$$\n",
    "\n",
    "$$g'(0) = f = \\underbrace{\\frac{g(h) - g(0)}{h}}_{\\mathrm{rank~2}} + \\mathcal{O}(h),$$\n",
    "\n",
    "but no **exact rank 2** exist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another example: Tucker\n",
    "Another attempt to avoid was the Tucker format \n",
    "\n",
    "(Tucker 1966, Lathauwer, 2000+), used first in statistics, where the data was given as a **data cube**.\n",
    "\n",
    "$$A(i,j,k) \\approx \\sum_{\\alpha \\beta \\gamma} G(\\alpha,\\beta,\\gamma)\n",
    "U_1(i,\\alpha)V(j,\\alpha)W(k,\\alpha)$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can compute Tucker by means of the SVD:\n",
    "- Compute <font color='red'> unfoldings </font>: $A_1, A_2, A_3$ \n",
    "- Compute left SVD factors: $A_i \\approx U_i \\Phi_i$\n",
    "- Compute the core: $G = A \\times_1 U^{\\top}_1 \\times_2 U^{\\top}_2\n",
    "  \\times_3 U^{\\top}_3$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main problem with Tucker format\n",
    "\n",
    "The main problem with the Tucker format is that it can not be used to represent **high-dimensional functions**\n",
    "\n",
    "We reduce $n^d$ complexity to $r^d$, which makes $d = 10,\\ldots, 20$ tracktable, but not more.\n",
    "\n",
    "Are there other ideas?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main motivation\n",
    "Matrices are good, so reduce everything to matrices! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Splitting into halves\n",
    "Slit index set into two, get $n^{d/2} \\times n^{d/2}$ matrix, compute its SVD:\n",
    "\n",
    "$$A(I, J) = \\sum_{\\alpha=1}^r U_{I, \\alpha} V_{J, \\alpha},$$\n",
    "\n",
    "then we have $d_1 +1 $ and $d_2 + 1$ - dimensional tensors. Do it recursively!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another construction\n",
    "Another construction, so-called Hierarchical Tucker, forms the construction:\n",
    "\n",
    "1. Compute Tucker decomposition, get a $r \\times r \\times \\ldots \\times r$ array;\n",
    "2. Merge indices by two, and compute **transfer matrices** of size $r \\times r \\times r$.\n",
    "3. Go up the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplest construction\n",
    "The simplest construction **separates** one index at a time and gives the **tensor-train** (or **matrix-product state**) decomposition\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) \\approx G_1(i_1) \\ldots G_d(i_d), $$\n",
    "\n",
    "where $G_k(i_k)$ is $r_{k-1} \\times r_k$ matrix for any fixed $r_k$.\n",
    "\n",
    "**Similar to Hidden Markov models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TT-format in a nutshell\n",
    "\n",
    "- If there is a canonical representation with rank $R$, then $r_k \\leq R$\n",
    "- TT-ranks are computable via SVD of auxiliary matrices\n",
    "- Basic linear algebra operations\n",
    "- Solve optimization problems with solution in the TT-format\n",
    "- **There is an exact interpolation formula** with $\\mathcal{O}(dnr^2)$ parameters (TT-cross method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TT-ranks are matrix ranks\n",
    " Define unfoldings: \n",
    " \n",
    " $A_k = A(i_1 \\ldots i_k; i_{k+1} \\ldots i_d)$, $n^k \\times n^{d-k}$\n",
    " matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "   Theorem: there exists a TT-decomposition with TT-ranks  \n",
    "   \n",
    "   $$r_k = \\mathrm{rank} A_k$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stability estimate\n",
    "No exact ranks in practice -- stability estimate! \n",
    "\n",
    "If $A_k = R_k + E_k$, $||E_k|| = \\varepsilon_k$\n",
    "  $$||\\mathbf{A}-\\mathbf{TT}||_F \\leq \\sqrt{\\sum_{k=1}^{d-1} \\varepsilon^2_k}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fast and trivial linear algebra\n",
    " Addition, Hadamard product, scalar product, convolution \n",
    " \n",
    " All scale linear in $d$ \n",
    " \n",
    " $$C(i_1,\\ldots,i_d) = A(i_1,\\ldots,i_d) B(i_1,\\ldots,i_d)$$\n",
    "    \n",
    "   $$C_k(i_k) = A_k(i_k) \\otimes B_k(i_k),$$\n",
    "   ranks are multiplied\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rounding\n",
    "**Rounding** procedure solves the following problem: given a TT-representation\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = G_1(i_1) G_2(i_2) \\ldots G_d(i_d)$$\n",
    "\n",
    "find **another one** with smaller memory that approximates it with **threshold** $\\epsilon$.\n",
    "\n",
    "It can be done in $\\mathcal{O}(dnr^3)$ operations using the **linear structure** of the format:\n",
    "\n",
    "We can do orthogonalization of the cores! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross approximation in $d$-dimensions\n",
    "\n",
    "Oseledets, Tyrtyshnikov, TT-cross approximation of multidimensional arrays, which generalizes the matrix result.\n",
    "\n",
    "**Theorem:** A rank-$r$ tensor can be recovered from $\\mathcal{O}(dnr^2)$ elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Packages\n",
    "We have two basic packages:\n",
    "- TT-Toolbox (http://github.com/oseledets/TT-Toolbox) which is a MATLAB Toolbox that implements all the basic algorithms \n",
    "- ttpy (http://github.com/oseledets/ttpy) - its Python counterpart\n",
    "- Tensor train on tensorflow (t3f) (https://github.com/Bihaqo/t3f)\n",
    "\n",
    "Many basic and advanced algorithms are implemented, and can be used as building blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swp: 0/9 er_rel = 1.9e+00 er_abs = 5.8e+08 erank = 7.0 fun_eval: 8400\n",
      "swp: 1/9 er_rel = 6.8e-16 er_abs = 2.1e-07 erank = 11.9 fun_eval: 36700\n",
      "This is a 10-dimensional tensor \n",
      "r(0)=1, n(0)=20 \n",
      "r(1)=2, n(1)=20 \n",
      "r(2)=2, n(2)=20 \n",
      "r(3)=2, n(3)=20 \n",
      "r(4)=2, n(4)=20 \n",
      "r(5)=2, n(5)=20 \n",
      "r(6)=2, n(6)=20 \n",
      "r(7)=2, n(7)=20 \n",
      "r(8)=2, n(8)=20 \n",
      "r(9)=2, n(9)=20 \n",
      "r(10)=1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tt\n",
    "from tt.cross.rectcross import cross\n",
    "d = 20\n",
    "n = 10\n",
    "x0 = tt.rand(d, n, 3)\n",
    "h = 1e-2\n",
    "def myfun1(x):\n",
    "    return np.sum(x, axis=1)\n",
    "    #return np.sqrt((((h * x) ** 2).sum(axis=1)))\n",
    "\n",
    "x1 = cross(myfun1, x0)\n",
    "x1 = x1.round(1e-8)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced methods\n",
    "\n",
    "Sometime the function is given only implicitly. How to construct an approximation?\n",
    "\n",
    "General strategy: formulate the problem as  a minimization problem\n",
    "\n",
    "$$ F(X) \\rightarrow \\min, \\quad X \\in \\mathcal{M}. $$\n",
    "\n",
    "- Linear systems: $F(x) = (Ax, x) - 2(f, x)$, where $x$ can be mapped to a tensor\n",
    "- Eigenvalue problems: $F(x) = (Ax, x)/(x, x)$\n",
    "- Dynamic problem: $f(x) = \\Vert \\frac{dx}{dt} - \\frac{dA}{dt} \\Vert$, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Minimization techniques\n",
    "- Idea 1: Use ordinary optimization method + projection: $X_{k+1} = P(X_k + G(X_k))$\n",
    "- Idea 2: Use alternating least squares (polylinear structure)\n",
    "- **Best: combine 1+2** (AMEN method).  Instead of optimizing $A \\approx UV^{\\top}$ we enrich the basis with new elements.  \n",
    "All of them are implemented in the Toolboxes (both Python and MATLAB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 5.405330241887626e-07\n",
      "Efficient rank: 35.31683561172893\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tt\n",
    "from tt.amen import amen_solve\n",
    "\n",
    "d = 8 \n",
    "f = 4\n",
    "mat = tt.qlaplace_dd([d] * f)\n",
    "rhs = tt.ones(2, d * f)\n",
    "sol = amen_solve(mat, rhs, rhs, 1e-6)\n",
    "print('Error:', (tt.matvec(mat, sol) - rhs).norm()/rhs.norm())\n",
    "print('Efficient rank:', sol.erank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications\n",
    "\n",
    "- There are many applications of tensor decompositions to UQ (Dolgov, Schleihl, Kazeev, Zhang, L. Daniel, Ballani, \n",
    "Grasedyck, Schwab, Khoromskij, ...)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
